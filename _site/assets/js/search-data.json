{"0": {
    "doc": "Acknowledgments",
    "title": "Course Acknowledgments",
    "content": "The content and structure of Data 101 have been designed and developed over the years thanks to many dedicated faculty and instructors at UC Berkeley. Valuable and essential instructional contributors (alphabetical order): Michael Ball, Joe Hellerstein, Lakshya Jain, Aditya Parameswaran, and Lisa Yan. ",
    "url": "/sp25/acknowledgments/#course-acknowledgments",
    
    "relUrl": "/acknowledgments/#course-acknowledgments"
  },"1": {
    "doc": "Acknowledgments",
    "title": "Syllabus",
    "content": "Academic Honesty policy and closing words adapted from Data 140. Course Culture inspired and adapted with permission from Grace O’Connell, the Asssociate Dean for Inclusive Excellence, and the Fall 2023 CS 39 Syllabus by Lisa Yan and Sarah Chasins . ",
    "url": "/sp25/acknowledgments/#syllabus",
    
    "relUrl": "/acknowledgments/#syllabus"
  },"2": {
    "doc": "Acknowledgments",
    "title": "Data Science Education",
    "content": "Interested in bringing the Data Science major or curriculum to your academic institution? If you are an instructor, please fill out this form if you would like support from Berkeley in offering some variant of our Data Science courses at your institution (or just to let us know that you’re interested). Information about the courses appear at data8.org and ds100.org. If you are only interested in learning Python or data science, please look at our Data 8 or Data 100 websites mentioned above. ",
    "url": "/sp25/acknowledgments/#data-science-education",
    
    "relUrl": "/acknowledgments/#data-science-education"
  },"3": {
    "doc": "Acknowledgments",
    "title": "Acknowledgments",
    "content": " ",
    "url": "/sp25/acknowledgments/",
    
    "relUrl": "/acknowledgments/"
  },"4": {
    "doc": "Announcements",
    "title": "Announcements",
    "content": "Announcements are stored in the _announcements directory and rendered according to the layout file, _layouts/announcement.html. ",
    "url": "/sp25/announcements/",
    
    "relUrl": "/announcements/"
  },"5": {
    "doc": "Announcements",
    "title": "",
    "content": " ",
    "url": "/sp25/announcements/",
    
    "relUrl": "/announcements/"
  },"6": {
    "doc": "Announcements",
    "title": "",
    "content": " ",
    "url": "/sp25/announcements/",
    
    "relUrl": "/announcements/"
  },"7": {
    "doc": "Announcements",
    "title": "",
    "content": " ",
    "url": "/sp25/announcements/",
    
    "relUrl": "/announcements/"
  },"8": {
    "doc": "Announcements",
    "title": "",
    "content": " ",
    "url": "/sp25/announcements/",
    
    "relUrl": "/announcements/"
  },"9": {
    "doc": "Announcements",
    "title": "",
    "content": " ",
    "url": "/sp25/announcements/",
    
    "relUrl": "/announcements/"
  },"10": {
    "doc": "Announcements",
    "title": "",
    "content": " ",
    "url": "/sp25/announcements/",
    
    "relUrl": "/announcements/"
  },"11": {
    "doc": "Announcements",
    "title": "",
    "content": " ",
    "url": "/sp25/announcements/",
    
    "relUrl": "/announcements/"
  },"12": {
    "doc": "Announcements",
    "title": "",
    "content": " ",
    "url": "/sp25/announcements/",
    
    "relUrl": "/announcements/"
  },"13": {
    "doc": "Announcements",
    "title": "",
    "content": " ",
    "url": "/sp25/announcements/",
    
    "relUrl": "/announcements/"
  },"14": {
    "doc": "Announcements",
    "title": "",
    "content": " ",
    "url": "/sp25/announcements/",
    
    "relUrl": "/announcements/"
  },"15": {
    "doc": "Announcements",
    "title": "",
    "content": " ",
    "url": "/sp25/announcements/",
    
    "relUrl": "/announcements/"
  },"16": {
    "doc": "Announcements",
    "title": "",
    "content": " ",
    "url": "/sp25/announcements/",
    
    "relUrl": "/announcements/"
  },"17": {
    "doc": "Announcements",
    "title": "",
    "content": " ",
    "url": "/sp25/announcements/",
    
    "relUrl": "/announcements/"
  },"18": {
    "doc": "Announcements",
    "title": "",
    "content": " ",
    "url": "/sp25/announcements/",
    
    "relUrl": "/announcements/"
  },"19": {
    "doc": "Announcements",
    "title": "",
    "content": " ",
    "url": "/sp25/announcements/",
    
    "relUrl": "/announcements/"
  },"20": {
    "doc": "Announcements",
    "title": "",
    "content": " ",
    "url": "/sp25/announcements/",
    
    "relUrl": "/announcements/"
  },"21": {
    "doc": "Announcements",
    "title": "",
    "content": " ",
    "url": "/sp25/announcements/",
    
    "relUrl": "/announcements/"
  },"22": {
    "doc": "Announcements",
    "title": "",
    "content": " ",
    "url": "/sp25/announcements/",
    
    "relUrl": "/announcements/"
  },"23": {
    "doc": "Assignment Tips",
    "title": "Assignment Tips",
    "content": "Author: Lisa Yan, Michael Ball, Rebecca Dang Last updated: October 25, 2024 (Fall 2024) . Quick links: . | Final Project Tips | . Jump to: . | Assignment Tips . | DataHub . | Working with Jupyter Notebooks | JupyterHub Keyboard Shortcuts | Navigating Long Notebooks | Split-screen Setup | Help! Why is my DataHub so slow? | Help! I can’t export my notebook to a PDF | . | Jupysql: PostgreSQL via ipython magic . | What is line/cell magic? | Making SQL queries in jupysql | Opening a database connection | Closing a database connection | . | psql, The PostgresSQL Interactive CLI . | Open a Terminal in DataHub | Opening a database connection | Closing a database connection | psql Meta-commands | Terminal commands | . | PostgreSQL details . | Postgres Connection URLs . | CLI Connection Arguments | . | DataHub’s local PostgreSQL Server | Catalog, Schema, Relation/Table, etc. | . | Local Setup | MongoDB debugging | . | . In most of this course, you will use DataHub to work on projects. PostgreSQL has a few quirks with DataHub that will be covered in this document. However, we strongly encourage you to check out the documentation as you work: . | The official PostgreSQL v14 documentation is great and can even be read cover-to-cover. | The jupysql documentation is the primary way you will be writing SQL commands for homework submission. | . Please see our policies on collaboration before working with any study groups. ",
    "url": "/sp25/assignment-tips/#assignment-tips",
    
    "relUrl": "/assignment-tips/#assignment-tips"
  },"24": {
    "doc": "Assignment Tips",
    "title": "DataHub",
    "content": "Working with Jupyter Notebooks . If you are new to using Jupyter Notebooks, please see the first lab assignment of Data 100 (course website). Data 101 assignments work very similarly; there are local and hidden autograder tests, the latter of which are run after you submit your assignment through Gradescope. Reminder about adding new cells: If you would like to add new cells, always do so before the cell in which you end up writing your answer. Failure to do so may break the auto-grader. JupyterHub Keyboard Shortcuts . First, to enter shortcut mode/exit editing mode, press Esc. This will then enable you to use any of the below keyboard shortcuts. | Operation | Keys | . | To enter shortcut mode/exit editing mode | Esc | . | Enter edit mode | Enter | . | Insert cell above | A | . | Insert cell below | B | . | Delete selected cell | D + D (Press D twice) | . | Undo cell operation | Z | . | Copy cell | C | . | Paste cell | V | . | Paste cell above | Shift + V | . | Redo | Ctrl + Shift + Z | . | Undo | Ctrl + Z | . Navigating Long Notebooks . JupyterLab includes an automatic table of contents on the left-hand controls that you can use to quickly jump to different sections of a notebook. Split-screen Setup . Want to splitscreen your JupyterHub? Simply drag a tab over to a different side of your JupyterHub. We recommend splitting your screen with your Jupyter notebook in one window, and a psql terminal in another window, like so (note these are two separate connections to the database!): . Help! Why is my DataHub so slow? . If you are encountering any of the following issues: . | Your SQL queries are taking a long time to run | Your SQL queries fail with No space left on device | Your DataHub is slow or unresponsive | Your browser window becomes slow or laggy | . Then, you may have run out of disk or memory space on your DataHub server. Here is a list of things you can do to fix this: . | Restart your current Jupyter kernel. To do so, go to Kernel -&gt; Restart Kernel and clear outputs of all cells. Then, refresh the page or navigate back to https://data101.datahub.berkeley.edu. | Check your display limits. If your row display limit is unbounded, then your notebook will crash. Clear all cell output and insert a cell close to the top of the notebook that sets the display limit to a more reasonable size, like %config SqlMagic.displaylimit = 50 . | Clear (all) cell outputs to reduce the amount of text your browser needs to load/render. In general, the total amount of content in a notebook can contribute to it feeling slow to navigate. If you see a lot of log output, for example, you can right click on the cell and clear just that cell’s output. | Close Unneeded Files &amp; Notebooks. Even files that aren’t actively displayed can have an impact on your current session. If you find yourself needing to jump between two tabs within JupyterLab, it may be more stable to open two browser tabs. (The best experience depends highly on the specifics of your browser, computer’s memory and other settings.) . | Close all Jupyter kernels other than the one that you are currently working on. To do so, go to the left sidebar and click on the stop button (dark circle with a light inscribed square). Go to Kernels, hover over each item, and hit the X button. Or, click “Shut Down All” with all notebooks still open, then manually start your kernel in the desired notebook. | Run VACUUM FULL. This command will instruct PostgreSQL to try to reclaim some space. To do so, run !psql postgresql://jovyan@127.0.0.1:5432/imdb -c 'VACUUM FULL' . | If your notebook is already unresponsive, you can run this command in a terminal window instead. To do so, go to File -&gt; New -&gt; Terminal. Then, run type psql postgresql://jovyan@127.0.0.1:5432/imdb -c 'VACUUM FULL' (notice that we don’t need to type ! in the terminal). | . | Drop and re-create your database. To do so, run the database setup commands in the beginning of your project notebook that includes the DROP DATABASE and CREATE DATABASE commands. To find out the size of all current databases, run . | In Jupyter notebook: !psql postgresql://jovyan@127.0.0.1:5432/ -c '\\l+' | In a Terminal outside of a psql session: !psql postgresql://jovyan@127.0.0.1:5432/ -c '\\l+' | In an active psql session: \\l+ | . If you are unable to drop the database, it might be because you have other open connections to the database. Run the below to terminate all other connections (replace imdb with your database name): . | In Jupyter Notebook, make a new cell. Make sure you replace the syntax with the desired database (e.g., below it is imdb) !psql postgresql://jovyan@127.0.0.1:5432/imdb -c 'SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE datname = current_database() AND pid &lt;&gt; pg_backend_pid();' | In psql, run the following command. Make sure you are conneted to the target database first. | To connect to imdb and delete all other imdb connections, use the meta-command \\c imdb. | Then, run SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE datname = current_database() AND pid &lt;&gt; pg_backend_pid(); | . | . | Restart your DataHub server. To do so, go to File -&gt; Hub Control Panel -&gt; Stop My Server. Then, refresh the page or navigate back to https://data101.datahub.berkeley.edu. | Restart your Web Browser. Sometimes, simply refreshing a browser tab can improve performance. However, completely quitting your browser can force it to clear up memory and release additional resources. If you don’t wish to close all tabs, then closing a tab (or window) and making a new one can also help with performance problems. | If none of the above work, please post on Ed with the following information: . | Your DataHub username (your @berkeley.edu name) | The project you are working on | The output of the following commands: | . | !df -h | !free -h | !psql -c 'SELECT pg_size_pretty(pg_database_size(current_database()))' | . | . Help! I can’t export my notebook to a PDF . If you are running the grader.export(...) cell and you receive an error that looks similar to LatexFailed: PDF creating failed, captured latex output which is preventing you from exporting your Jupyter notebook to a PDF, this is likely because there are unescaped LaTeX special characters (such as \\, {, }, _) in your notebook. Things you can do to troubleshoot: . | Read the entire error message carefully. It is probably very long, but somewhere in the message it will say which text in your notebook is causing the error. Here is an example excerpt from an error message: ! Undefined control sequence. l.349 ...r materialized view is much smaller\\faster than for ? ! Emergency stop. l.349 ...r materialized view is much smaller\\faster than for . | The special character \\ is reserved in LaTeX and caused the issue in this case. To fix this, use / instead. | . | Make sure that the cells you made or modified are Markdown or Code cells, rather than Raw cells. You can change the type of your cell by clicking on the cell to select it, and then clicking on the dropdown at the top of your notebook to change the cell type. | . ",
    "url": "/sp25/assignment-tips/#datahub",
    
    "relUrl": "/assignment-tips/#datahub"
  },"25": {
    "doc": "Assignment Tips",
    "title": "Jupysql: PostgreSQL via ipython magic",
    "content": "What is line/cell magic? . Before getting started, read about line magic (%) and cell magic (%%) on TutorialsPoint. These commands will be used extensively in this project and future projects to aid us in running SQL queries. In Jupyter Notebooks, a cell ‘magic’ command is a special command that is preceded by two percentage signs (%%). Cell magics operate on entire cells and are used to change the behavior of the entire cell. They are not part of the Python language itself but are specific to the Jupyter environment. They help us do a lot of cool things, like run SQL commands directly within Jupyter! For some questions with cell magic, we will be saving the literal query string with query snippets using --save, as illustrated below: . %%sql --save query_result &lt;&lt; SELECT * FROM table ... To call SQL commands, we use the Python package jupysql. We strongly recommend you check out the jupysql documentation. It has a lot of hidden gems! . To load jupysql, run: . %load_ext sql . You will often seen this written as the following, which lets you reload the extension multiple times if there is an issue. %reload_ext sql . Making SQL queries in jupysql . Here are the two ways of writing a SQL query and storing the query result into a Python variable result: . | Single-line magic: result = %sql SELECT * FROM table ... | Multi-line cell magic: | . %%sql result &lt;&lt; SELECT * FROM table ... Opening a database connection . Before running any SQL queries, you must have a working connection to a database on a Postgres server. It usually looks something like this, which connects to the Postgres server (at the local IP address, using the user jovyan) and the database imdb. %sql postgresql://jovyan@127.0.0.1:5432/imdb . Closing a database connection . You may sometimes wnat to close the database connection, in case you want to delete your database and start from a new copy. To close the connection, you can either restart your kernel or explicitly run the following in its own cell: . %sql --close postgresql://jovyan@127.0.0.1:5432/imdb . If that’s not working, see the bottom of this page for how to relaunch your DataHub instance. ",
    "url": "/sp25/assignment-tips/#jupysql-postgresql-via-ipython-magic",
    
    "relUrl": "/assignment-tips/#jupysql-postgresql-via-ipython-magic"
  },"26": {
    "doc": "Assignment Tips",
    "title": "psql, The PostgresSQL Interactive CLI",
    "content": "The psql program is the PostgreSQL interpreter and CLI, or Command-Line Interface. Knowing psql is very useful to understand what your database looks like, execute meta-commands, and explore quick queries. Just like when you type python in a Terminal, psql’s primary use is interactively run queries and commands against a database. Open a Terminal in DataHub . To open a Terminal in DataHub, Navigate to Data101’s DataHub, then go to File → New → Terminal. Note: Do not open a Terminal on your local machine; it does not know how to connect to DataHub’s server, much less your DataHub’s Postgres server! . Opening a database connection . Use either command to connect to the imdb database, if it has been created: . psql postgresql://127.0.0.1:5432/imdb . psql -h localhost -d imdb . If no database has been created: . | You will likely get this error: psql: error: connection to server at '127.0.0.1', port 5432 failed: FATAL: database \"imdb\" does not exist\" | In this case, you can still connect to the server and list databases, etc., as follows: . psql -h localhost . | However, you won’t be able to see any relations, because this default connection cannot access what’s in imdb. | To create the imdb database, see the corresponding Jupyter notebook and run the cells that contain commands such as CREATE DATABASE. For Fall 2024, imdb is created in the Project 1 notebook. You may which to run createdb -h localhost [dbname] to make a new, empty database. | . Closing a database connection . \\q: This exits out of the psql program and also closes your current connections. \\c &lt;databasename&gt;: This keeps your psql client open, closes your current database conection, and opens a connection to &lt;databasename&gt;. psql Meta-commands . psql meta-commands doc: list . | Meta-Command | Description | . | \\? | Help | . | \\l | Lists databases | . | \\d | Lists relations | . | \\d tablename | List schema of the relation tablename. | . | \\q | Quit psql | . | \\x auto | Expanded view of records. To toggle off, \\x off. | . Making queries: You can write queries in psql! To write queries that span multiple lines, simply use the newline key (i.e., &lt;Return&gt;). However, to execute a query in psql, you must use the semicolon. This is generally good style, anyway! . Display screen: If a query’s result will span more than the available display screen, psql will launch a different display screen. You can navigate this screen by pressing &lt;space&gt;, &lt;return&gt; to display more, up/down arrows to scroll up and down, or use the page up/page down keys, and q to exit the query view. Terminal commands . Here are some Terminal shortcuts to help you better navigate psql. (These commands are standard across all Unix terminal envrionments, including macOS.) . | Keys | Description | . | ^ c | Cancel current operation | . | ^ a | Jump to beginning of line | . | ^ e | Jump to end of line | . | ^ left | Jump to previous word | . | ^ right | Jump to next word | . | space | If currently exploring a query result, see more of the result. | . | q | If currently exploring a query result, exit from the result. | . ^ is the symbol for the control key. ",
    "url": "/sp25/assignment-tips/#psql-the-postgressql-interactive-cli",
    
    "relUrl": "/assignment-tips/#psql-the-postgressql-interactive-cli"
  },"27": {
    "doc": "Assignment Tips",
    "title": "PostgreSQL details",
    "content": "Postgres Connection URLs . Throughout this course you will see a number of ways to connect to a database server. These connection strings are like URLs, but won’t work in a web browser. Instead tools like SQLAlchemy in Python (and others in Ruby, Java, etc.) use the same URL format to establish a connection to a database server, whether it’s on a local machine or connected to the internet. Consider the following: . postgresql://jovyan@127.0.0.1:5432/imdb . This has the following components: . [db driver]://[username][:password]@[server address][:port]/[database name] . In many cases, parts of these connection URIs, like the port, username, or database can ommitted if using the defaults. CLI Connection Arguments . All of the default tools like psql, createdb, dropdb, that come with a PostgreSQL installation, and many other tools allow you to specify the connection more explicitly, which can be convenient when using the command line. This is the equivalent to the previous URL method: . psql -h localhost -d imdb . | psql -h localhost -d imdb -U jovyan -p 5432 would be equivalent to passing all of the arguments in the connection URL. | On DataHub, you will always need to specify -h localhost to connect to the locally running server. | Passing -d is usually done for convenience, but is not required. In this case you will not be connected to a specific database. | -U will default to joyvan on DataHub, but is otherwise the current user. | -p has a default value of 5432 for Postgres. | . Try running psql --help for more explanation. (Outside of this class, you may find the need to provide a password, especially when connecting to a remote server.) . DataHub’s local PostgreSQL Server . Every time you want to work with a database, you need to connect to a database server. On the Data101 DataHub, a Postgres server is automatically started in the background: . postgresql://jovyan@127.0.0.1:5432/imdb . | connect using postgresql as the database driver | connect using JupyterHub username joyvan (why is the default username? see here Jupyter, JupyterHub) | 127.0.0.1 is a “loopback” IP address representing the current machine, which is also mapped to hostname localhost. | connect to the database imdb on this server. Note that if the imdb database has not yet been created, this connection may fail. | . Catalog, Schema, Relation/Table, etc. | StackOverflow: Catalog, Schema, Relation/Table differences . | pg_toast: TOAST storage schema documentation 73.2 | pg_catalog: System catalog schema documentation 5.9.5 | . ",
    "url": "/sp25/assignment-tips/#postgresql-details",
    
    "relUrl": "/assignment-tips/#postgresql-details"
  },"28": {
    "doc": "Assignment Tips",
    "title": "Local Setup",
    "content": "While you are welcome to set up everything locally, when grading we will assume that your submission was developed on DataHub. If you would like to develop locally, please make sure you have the following installed: . | otter-grader==5.1.3 | jupysql==0.10.0 | pgspecial=1.13.1 | psycopg==3.2.1 | mongodb | PostgreSQL server. For Mac, you can use Postgres.app, or homebrow (brew install postgresql). | . Either way, we recommend you always work on DataHub, as staff will not be able to debug/support local setup issues. ",
    "url": "/sp25/assignment-tips/#local-setup",
    
    "relUrl": "/assignment-tips/#local-setup"
  },"29": {
    "doc": "Assignment Tips",
    "title": "MongoDB debugging",
    "content": "To prevent bracket mismatches while creating your queries, it is recommended to turn on “Auto Close Brackets” via Settings in JupyterHub. Furthermore, since we are using Python dictionaries as our query filter, make sure to wrap all keys and values inside quotes. ",
    "url": "/sp25/assignment-tips/#mongodb-debugging",
    
    "relUrl": "/assignment-tips/#mongodb-debugging"
  },"30": {
    "doc": "Assignment Tips",
    "title": "Assignment Tips",
    "content": " ",
    "url": "/sp25/assignment-tips/",
    
    "relUrl": "/assignment-tips/"
  },"31": {
    "doc": "Weekly Schedule",
    "title": "Weekly Schedule",
    "content": "Add to Google Calendar . ",
    "url": "/sp25/calendar/",
    
    "relUrl": "/calendar/"
  },"32": {
    "doc": "Final Project Tips",
    "title": "Final Project Tips and Resources",
    "content": "⚠️ The world of data engineering is vast. We don’t have experience with all, or even most of these tools. Part of the goals of this are to help you learn how to learn new tools. This is not an exhaustive guide; but might serve as a useful set of pointers to other tools. ",
    "url": "/sp25/assignments/final-project-tips/#final-project-tips-and-resources",
    
    "relUrl": "/assignments/final-project-tips/#final-project-tips-and-resources"
  },"33": {
    "doc": "Final Project Tips",
    "title": "Table of contents",
    "content": ". | Installing Software Locally . | Postgres &amp; Homebrew | Postgres (Mac OS X only) | MongoDB | . | Non-Relational DBs | ER Diagrams . | Tools for Creating ER Diagrams | . | GitHub &amp; JupyterHub | Using SQL &amp; Python | Sharing SQL Databases | Writing Tips and Compiling PDFs . | Scientific Writing and markdown . | Rendering Markdown with Pandoc or Quarto | . | Markdown Guides | . | . ",
    "url": "/sp25/assignments/final-project-tips/#table-of-contents",
    
    "relUrl": "/assignments/final-project-tips/#table-of-contents"
  },"34": {
    "doc": "Final Project Tips",
    "title": "Installing Software Locally",
    "content": "We recommend everyone install Postgres locally, following their documentation. Postgres &amp; Homebrew . Homebrew (brew) is a package manager for macOS (and Linux) that simplifies the installation of software. You might find brew useful since it can install many different pieces of software. Checkout https://brew.sh . However, if you’re comfortable with Conda, you can also use that. Otherwise, we tend to find brew to be easier on macOS. To install PostgreSQL using Homebrew: . | Run the following command in your terminal: brew install postgresql | After installation, you can start the PostgreSQL service with: brew services start postgresql | And stop it with: brew services stop postgresql | . Postgres (Mac OS X only) . If you are having trouble with database permissions using a brew installation of Postgres, we suggest installing Postgres.App which is a full-featured application native to Mac OS X only. It installs a simple server (with GUI management) and psql program; we recommend uninstalling any existing versions of postgres if you decide to go this route. MongoDB . We recommend everyone install MongoDB locally, following their documentation. Some high-level steps: . Install the Community Edition of Mongo: . | Install MongoDB CommunityEdition | Install mongosh, the MongoDB shell | Once installed, follow the instructions to run a local Mongo server. | Then verify that you can connect a Mongo client (e.g., mongosh). | Finally, install needed Python packages (e.g., pymongo) if that’s what you’ll regularly use to execute mongo commands. | . ",
    "url": "/sp25/assignments/final-project-tips/#installing-software-locally",
    
    "relUrl": "/assignments/final-project-tips/#installing-software-locally"
  },"35": {
    "doc": "Final Project Tips",
    "title": "Non-Relational DBs",
    "content": ". | Cassandra: A highly scalable, high-performance distributed database designed to handle large amounts of data across many commodity servers. Documentation | Redis: An in-memory key-value store known for its speed and versatility. Documentation | CouchDB: A database that uses JSON for documents, JavaScript for MapReduce queries, and regular HTTP for an API. Documentation | Neo4j: A graph database that uses graph structures for semantic queries with nodes, edges, and properties. Documentation | DynamoDB: A fully managed proprietary NoSQL database service provided by AWS. Documentation | . ",
    "url": "/sp25/assignments/final-project-tips/#non-relational-dbs",
    
    "relUrl": "/assignments/final-project-tips/#non-relational-dbs"
  },"36": {
    "doc": "Final Project Tips",
    "title": "ER Diagrams",
    "content": "Tools for Creating ER Diagrams . | Lucidchart: A web-based diagramming tool that allows you to create ER diagrams and other types of diagrams. Website | Freeform App: A (collaborative) iPad and Mac app which allows you to draw diagrams and take notes. It’s not specific to ER diagrams, but there’s a good chance you already have it. Website | dbdiagram.io: An online tool for creating database diagrams by writing code. Website | . ",
    "url": "/sp25/assignments/final-project-tips/#er-diagrams",
    
    "relUrl": "/assignments/final-project-tips/#er-diagrams"
  },"37": {
    "doc": "Final Project Tips",
    "title": "GitHub &amp; JupyterHub",
    "content": "Because your repository is private, we recommend setting up SSH if you are working on JupyterHub. ",
    "url": "/sp25/assignments/final-project-tips/#github--jupyterhub",
    
    "relUrl": "/assignments/final-project-tips/#github--jupyterhub"
  },"38": {
    "doc": "Final Project Tips",
    "title": "Using SQL &amp; Python",
    "content": "Using psycopg3 to Connect to PostgreSQL, Create a Table, and Insert Data . To interact with PostgreSQL using Python, you can use the psycopg3 library. Below is a short example demonstrating how to connect to a database, create a table, and insert some data. First, install the psycopg3 library if you haven’t already: For more information on psycopg3, you can refer to the following resources: . | Official Documentation: psycopg3 Documentation (Start here!) | API Reference: psycopg3 API Reference | GitHub Repository: psycopg3 on GitHub | . pip install psycopg[binary] . Then, use the following Python code: . import psycopg # Connect to your PostgreSQL database # Make sure it is already running on your computer. # (This is done by default on JupyterHub) # Assuming the database mydb exists... conn = psycopg.connect(\"postgresql://localhost/mydb\") # Create a cursor object cur = conn.cursor() # Create a table cur.execute(\"\"\" CREATE TABLE IF NOT EXISTS users ( id SERIAL PRIMARY KEY, name TEXT, age INT ) \"\"\") # Insert some data into the table cur.execute(\"\"\" INSERT INTO users (name, age) VALUES ('Alice', 30), ('Bob', 25), ('Charlie', 35) \"\"\") # Insert data using safe query interpolation cur.execute( \"INSERT INTO users (name, age) VALUES (%s, %s)\", ('David', 28) ) # Insert data using a prepared statement # See also: https://www.postgresql.org/docs/8.1/sql-syntax.html#AEN1368 prepared_statement = \"INSERT INTO users (name, age) VALUES (%s, %s)\" cur.execute(prepared_statement, ('Eve', 22)) # Insert data from a list user_list = [ ('Frank', 40), ('Grace', 29), ('Hannah', 33) ] # Use executemany to insert multiple rows # This properly escapes quotes, etc. cur.executemany( \"INSERT INTO users (name, age) VALUES (%s, %s)\", user_list ) # Commit the transaction conn.commit() # Close the cursor and connection cur.close() conn.close() . ",
    "url": "/sp25/assignments/final-project-tips/#using-sql--python",
    
    "relUrl": "/assignments/final-project-tips/#using-sql--python"
  },"39": {
    "doc": "Final Project Tips",
    "title": "Sharing SQL Databases",
    "content": "You will likely find that databases are hard to share among teammates. A few suggestions: . | To make data processing reproducible, maintain a separate notebook/set of scripts that teammates can use to reproduce the same SQL database. This ideally is a separate file from any EDA/task-writing that your team does. It may contain some data wrangling functions needed to load in the same data. You may assume that your teammates have already pre-downloaded in the raw files. | If you are randomly sampling the data, set a random seed (e.g., in Python) so that anyone who follows the same steps will reproduce the identical set of rows across different tables. | To make database export/import easy, use pg_dump to create a .sql file. The below takes a yelp_db database exists on your Postgres server and dumps the full database (schema + records) into a file called yelp.sql: . pg_dump --encoding utf8 yelp_db -f yelp.sql . You can then share the .sql binary file directly with your teammates. However, this method does not lead to good data provenance (and it also can assume specific features about a given Postgres server setup), so we generally recommend sharing both this binary AND the steps used to recreate the database from raw data files. | . ",
    "url": "/sp25/assignments/final-project-tips/#sharing-sql-databases",
    
    "relUrl": "/assignments/final-project-tips/#sharing-sql-databases"
  },"40": {
    "doc": "Final Project Tips",
    "title": "Writing Tips and Compiling PDFs",
    "content": "You are welcome to generate your report PDF in any way you choose. If you want to write collaboratively, we would personally recommend against writing in a Jupyter Notebook. However, the very rough markdown file we’ve provided should be easy enough to avoid git conflicts. Otherwise, you are welcome to write in Google Docs or Overleaf / LaTeX if you’d like. Remember: A particular format for your report is not required. You do not need to be extra fancy; simply focus on the clarify of your writing. Scientific Writing and markdown . So, you’ve got a Markdown document, how do you render it as a PDF? There’s plenty of options! Each tool may be suited to more complex tasks in the long0ryb, but here’s a few places to get started. Rendering Markdown with Pandoc or Quarto . Pandoc is a swiss army knife of document conversion tools, and it’s already installed on DataHub. | To install pandoc yourself: brew install pandoc | Basic compilation with pandoc: pandoc file.md -t [format] -o output-filename where [format] is most likely pdf for your use cases. | Don’t forget to run pandoc --help. | . Quarto is another useful tool designed for publishing scientific documents and websites. It can similarly render markdown files to PDFs, and has many themes and advanced styling options, including the ability to run code in a markdown file and render its outputs in the final PDF. | Install quarto: brew install quarto | render markdown to PDF: quarto render file.md --to pdf | Likewise, run quarto render --help | . Markdown Guides . If you’d like to learn more about writing Markdown, you can checkout these guides: . | GitHub Flavored Markdown | Pandoc’s Markdown | Quarto’s Markdown Guide | . The vast majority of the text you write in markdown will not vary by the tool you use. However, different tools may offer specific features. ",
    "url": "/sp25/assignments/final-project-tips/#writing-tips-and-compiling-pdfs",
    
    "relUrl": "/assignments/final-project-tips/#writing-tips-and-compiling-pdfs"
  },"41": {
    "doc": "Final Project Tips",
    "title": "Final Project Tips",
    "content": " ",
    "url": "/sp25/assignments/final-project-tips/",
    
    "relUrl": "/assignments/final-project-tips/"
  },"42": {
    "doc": "Optional Final Project",
    "title": "Optional Final Project",
    "content": " ",
    "url": "/sp25/assignments/final-project/",
    
    "relUrl": "/assignments/final-project/"
  },"43": {
    "doc": "Optional Final Project",
    "title": "Table of contents",
    "content": ". | Overview (Instructions from Fa24, will be updated for Sp25) . | Learning Goals | . | Pick your project scope: a dataset and second system . | :memo: Suggested Default Project | Choosing an alternate dataset | Another system and/or application | . | Get Started . | Template Repository | Setup and install your tools | . | Design Your Dataset Schema | Load Your Dataset into PostgreSQL and Another Data System . | Data Loading, Cleaning, and Sampling | . | Design and Analyze a Benchmark Set of Tasks | Present Your Findings . | Report Components | . | Project Deliverables . | 1. Team Matching Survey (due TBD) | 2. Checkpoint: Checkpoint Report and GitHub Repository (due TBD) | 3. Final Report and GitHub Repository (due TBD) | 4. Team Member Assessment (due TBD) | . | Grading | Academic Integrity and Collaboration Policy | Working in Groups | FAQ | Acknowledgments | . ",
    "url": "/sp25/assignments/final-project/#table-of-contents",
    
    "relUrl": "/assignments/final-project/#table-of-contents"
  },"44": {
    "doc": "Optional Final Project",
    "title": "Quick Links",
    "content": ". | Jump to Getting Started | Final Project Tips | Template repository | Checkpoint Gradescope Submissions: . | Checkpoint Submission [Group] | Checkpoint Peer Assessment [Individual] | Final Submission [Group] | Peer Assessment [Individual] | . | . ",
    "url": "/sp25/assignments/final-project/#quick-links",
    
    "relUrl": "/assignments/final-project/#quick-links"
  },"45": {
    "doc": "Optional Final Project",
    "title": "Overview (Instructions from Fa24, will be updated for Sp25)",
    "content": "The final project is optional; if you submit it, it will be 10% of your final grade. The final project will be a multi-part group project which will ask that you complete an investigation of two data systems—one of which is PostgreSQL and the other will be a non-relational system of your choice—based on a performance benchmark (dataset, queries, and measurements) of your design. If you are interested in completing this optional final project, we encourage you to skim this page and start finding groups for the Team Matching Survey deadline on TBD. A Project Partner search thread has also been posted on Ed. Group confirmations will be released on TBD. You will submit a GitHub repository and written report, where your analysis will include the following steps: . | Design your dataset schema. Design a relational schema for a chosen dataset. Draw an ER diagram for your schema. | We have curated a list of suggested datasets to use for your project, but you are also welcome to select an external dataset: . | Yelp: https://www.yelp.com/dataset | Open Library: https://openlibrary.org/developers/dumps | Wikipedia: https://dumps.wikimedia.org/ . | You will likely want to start with a recent dump of English Wikipedia. (https://dumps.wikimedia.org/enwiki/20241101/) | We do not recommend that you consider the full text/HTML dump of Wikipedia for this assignment, and only the metadata about the articles. However, you could choose to sample the text dump. | Wikipedia maintains a guide on their own data dumps. | . | If you would like to “bring your own” dataset, see the section “Choosing an alternate dataset” for more information. | . | . | Load your dataset into PostgreSQL. Given the size of your dataset, instead of DataHub you may need to do a local installation of JupyterHub and PostgreSQL &gt;14. More installation details to follow. | Load your dataset into a non-relational data system. Some examples could include: MongoDB, Graph databases (neo4j), Streaming databases (Flink, Kafka), key-value store (Redis, memcached), Spark, Velox, Polars, Apache Data Fusion, Ibis. Be creative here! If you need help with picking a data system, or other feedback in general, feel free to post on Ed. | Design and analyze a benchmark set of tasks. Select a representative set of 5 queries (i.e., tasks) that compare the two data system on the following dimensions: . | Fitness/ergonomics: Is your data system well-designed for the task? How easy is it to complete this task and/or can it even do the task? (i.e., does the system aid you in writing the queries you are trying to write) | Performance: How much disk storage or memory (RAM) and time resources does each system spend to complete this task? | . | Present your findings in a written report. This report could be used in an imaginary scenario where, say, a manager or labmate could use your report to determine which of two data systems to use. Your report therefore should include diagrams, code snippets, tables/visualizations (where appropriate), and broader reflections/recommendations about the usability of the systems themselves (ease of setup/debugging, etc.). | You are also required to submit a project checkpoint. For the checkpoint, we expect you to cover enough setup/groundwork such that you can run at least one query in PostgreSQL on your chosen dataset. | . | . Group work requirement: You must be in a team of 3 or 4. If you’re looking for a group OR if you have a partial/full team, everyone who is looking to do the final project must fill out this Team Matching Survey by TBD. Group confirmations will be sent out by TBD. Limited exceptions will be made for personal circumstances, or senior/honors thesis work; see the Project Deliverables section for more details. Learning Goals . This project is designed to give you a “real-world” example of working with an unknown dataset, and doing so in a team. We do not expect you to have all the answers, and there are a number of tasks which are not directly covered in class. You should expect to get practice with: . | Software Engineering: . | Setting up new tools on your own computer and debugging the error messages that (sometimes) happen | Reading documentation for new tools | Working with git and GitHub | . | Data Engineering: . | Interpreting an unknown dataset | Designing a schema / modeling a database | Analyzing professional data systems | . | Professional Skills: . | Working in groups effectively | Writing and communicating technical concepts | . | . Jump to Top . ",
    "url": "/sp25/assignments/final-project/#overview-instructions-from-fa24-will-be-updated-for-sp25",
    
    "relUrl": "/assignments/final-project/#overview-instructions-from-fa24-will-be-updated-for-sp25"
  },"46": {
    "doc": "Optional Final Project",
    "title": "Pick your project scope: a dataset and second system",
    "content": "The goal of this project is to create a performance benchmark to analyze and study two different systems: PostgreSQL and another system of your choice. A performance benchmark is often a standard set of tasks performed on a given workload. In your project, you will define a workload as your dataset, and you will define tasks as queries, materializations, deletions, pipelined tasks—anything that will give you more information about how the systems work. Because of the open nature of this project (choice of second data system, workload, and tasks), for this first version of the project, we suggest a default project below to constrain your search space. However, you should definitely feel free to explore other options beyond the default project—particularly if you came across a cool system or dataset in lecture or in the wild! . Please see the FAQ if you are on the fence about doing the final project. :memo: Suggested Default Project . We suggest the following choices for most project teams: . | PostgresSQL | Dataset: Yelp | Compare to: MongoDB | . This default project still leaves a lot of room for your team to decide on the set of benchmark tasks. You can also decide what additional entities to build into your project. We recommend looking at users, businesses and reviews if you don’t know where to start. For example, each user has a list of friends which could make for some interesting comparisons between SQL and NoSQL systems. Reducing dataset complexity: The Yelp dataset (Main, not Photos) is relatively large, about ~10GB when uncompressed. You do not need to load all of the records into your Postgres (or MongoDB) databases. We recommend sampling the Yelp dataset and/or choosing a subset of the database schema to make working with the dataset easier (see Final Project Tips). In your written report, you will need to demonstrate that the reduced dataset that you construct for your workload must meet the minimum requirements listed in the Choosing an alternate dataset section below. Choosing an alternate dataset . If you’d like to pick a different dataset, you are welcome to do so. However, it must meet all of the following requirements: . | Total uncompressed size ≥ 1GB of data | Minimum of 1 million records (across all entities) | Minimum of 3 distinct types of entities (e.g. users, resources, reviews) | Each entity must have at 2 non-primary/non-foreign key attributes | There must be at least 2 foreign key relationships (e.g. users who leave reviews, and reviews which belong restaurants) | . The data set must be something which is appropriate to write about in a report which is read by the course staff and your teammates. It cannot have “confidential” data (e.g. from a business). In your written report you will need to demonstrate that your dataset meets these requirements. How might you decide if an alternate dataset makes sense? If you are looking for alternate datasets, it can be helpful to pick a domain in which you have expertise or deep interest amongst your teammates. Otherwise, you should consider the clarity and organization of the raw data. We’ve suggested datasets which should minimize the amount of the needed to be spent cleaning and interpreting data—though some of that work is always necessary. Look for data which are well documented, have clearly named attributes, and come in easy-to-parse formats like JSON, CSV or YAML. Another system and/or application . You’ve already gained experience with Postgres; now you must pick a different data system or application to explore. The data system should help you accomplish some tasks which are different from that of a typical relational database. Some examples could include: MongoDB, Graph databases (neo4j), Streaming databases (Flink, Kafka), key-value store (Redis, memcached), Spark, Velox, Polars, Apache Data Fusion, Ibis, Apache airflow, dbt. Be creative here! If you need help with picking a data system, or other feedback in general, feel free to post on Ed. Start by exploring your dataset and asking: “Where might a RDMS not be well-suited for storing, analyzing, or processing this dataset?” This will help guide you towards a tool that will lead to more interesting insights. How do you decide on a tool/system before using it? Read the docs! Watch some videos. Do a brief search for problems, errors, or user communities. Ideally, you will be able to work with and learn about a tool which has excellent documentation and for which it is easy to search for answers to questions. Much of this research can all happen before you even install the tool. No one is expected to learn new tools simply by guessing the right commands; learning how to use the documentation provided will make the process much easier! . You should also consider picking a tool based on its fitness and ergonomics for the tasks you want to accomplish. See the Design and Analyze a Benchmark Set of Tasks for more information. Jump to Top . ",
    "url": "/sp25/assignments/final-project/#pick-your-project-scope-a-dataset-and-second-system",
    
    "relUrl": "/assignments/final-project/#pick-your-project-scope-a-dataset-and-second-system"
  },"47": {
    "doc": "Optional Final Project",
    "title": "Get Started",
    "content": "Template Repository . To get started, one team member needs to create a private GitHub repository. Start from the DATA 101 Final Project Template (​https://github.com/new?owner=cal-data-eng\\&amp;template_name=final-proj\\&amp;template_owner=cal-data-eng) . Your project repo should be private. After creating your repository, add your teammates as collaborators. The repository we’ve provided is a light scaffold to get you started. You are free to adapt it for your team’s needs as necessary. However, be careful not to commit very large files to git. Setup and install your tools . We expect each team member to install tools on their local machines. DataHub is available as a backup. Final Project Tips includes some tips for installing tools. For most tools, like PostgreSQL or MongoDB, you should get started by following the installation instructions in the official documentation. ",
    "url": "/sp25/assignments/final-project/#get-started",
    
    "relUrl": "/assignments/final-project/#get-started"
  },"48": {
    "doc": "Optional Final Project",
    "title": "Design Your Dataset Schema",
    "content": "Review the documentation for your chosen dataset. Your job will be to design a relational schema for your dataset. You will probably want to inspect the raw documents to determine the type (e.g., integer, text) for each attribute. Note: We do not require that your schema be purely relational. You may choose to make use of PostgreSQL’s support for jsonb and array column types. You may choose to simply the suggested datasets based on the type of analysis you do. However, any simplification must still meet the minimum requirements for datasets as listed in the section “Choosing an alternate dataset”. A good schema will have clear names for tables and columns following the DATA 101 SQL Style Guide, and will use primary and foreign keys as appropriate. In your schema you should also consider what columns should be indexed, and whether any additional constraints (like NOT NULL) would be helpful. We suggest you start not by writing the DDL, but by mapping out a (simplified) ER Diagram. Tools like Lucidchart or Google Drawing might be helpful. You can also try tools like dbdiagram.io which allow you to use SQL to generate the diagram if you prefer to work that way! . The project checkpoint will ask for just one of your schema or the ER Diagram, but the final report will ask that you submit both components. It is totally OK to adapt your schema as you learn more about your dataset, and for the schemas included in your checkpoint and final reports to not be exactly the same. Jump to Top . ",
    "url": "/sp25/assignments/final-project/#design-your-dataset-schema",
    
    "relUrl": "/assignments/final-project/#design-your-dataset-schema"
  },"49": {
    "doc": "Optional Final Project",
    "title": "Load Your Dataset into PostgreSQL and Another Data System",
    "content": "Data Loading, Cleaning, and Sampling . If you sample, sample down to roughly the additional dataset parameters (1 GB, ~1 million total records). Review the lecture on sampling for some example code. You can choose whether to sample a dataset before or after loading it into your database. The final project tips page also includes some guidance on using Python to interact with a Postgres database using the library psycopg. You are also welcome to copy code from any of your project notebooks to help you get started. Otherwise, there will be Ed threads dedicated to each tool and dataset where you can ask questions and share code. ",
    "url": "/sp25/assignments/final-project/#load-your-dataset-into-postgresql-and-another-data-system",
    
    "relUrl": "/assignments/final-project/#load-your-dataset-into-postgresql-and-another-data-system"
  },"50": {
    "doc": "Optional Final Project",
    "title": "Design and Analyze a Benchmark Set of Tasks",
    "content": "In this assignment, you will need to decide a benchmark set of tasks. Select a representative set of 5 queries (i.e., tasks) that compare the two data system on the following dimensions: . | Fitness/ergonomics: Is your data system well-designed for the task? How easy is it to complete this task and/or can it even do the task? (i.e., does the system aid you in writing the queries you are trying to write) | Performance: How much disk storage or memory (RAM) and time resources does each system spend to complete this task? | . Other requirements for the five tasks: . | 3 SQL queries should be runnable on Postgres. | 2 queries should be runnable on your second system. | Ideally, all five tasks can be run on both systems so that you can compare both fitness/ergonomics and performance. However, you may find midway through that, say, some SQL queries can’t be implemented in the second system, or vice versa. See the apples to oranges section below. | . Choosing tasks The tasks you select should help to best demonstrate the capacities of systems you are comparing. For example, consider not just the execution time of an individual query, but the time that it took to load data in, the memory (RAM) or storage uses of that particular system. If you were to consistently integrate new data, how would this system perform? What parameters does each system provide that allow you to improve performance of a given task or workload? Etc. Fitness and “ergonomics” The fitness of a tool describes how well suited a tool is to a specific task. When comparing tools, one of your tasks will be to compare their fitness for the particular problem you are trying to solve. For example, both MongoDB and PostgreSQL can store and query semi-structured data, but they take very different approaches. When engineers talk about “ergonomics” (or affordances) they are referring to whether a particular tool makes it easy or intuitive to accomplish a specific task. One example: Jupyter Notebooks provide good ergonomics for exploring data. Another example: SQL databases make JOINing data convenient, but MongoDB does not. Comparing apples to oranges It can be tricky to compare a RDBMS to a NoSQL DB when they are designed to solve different problems. Think holistically about the comparisons, both for fitness, ergonomics, and performance. Ultimately, if a task cannot be implemented in one system, you should still reflect on why the task selection was a good choice, and what barriers/constraints you found in the other system. Jump to Top . ",
    "url": "/sp25/assignments/final-project/#design-and-analyze-a-benchmark-set-of-tasks",
    
    "relUrl": "/assignments/final-project/#design-and-analyze-a-benchmark-set-of-tasks"
  },"51": {
    "doc": "Optional Final Project",
    "title": "Present Your Findings",
    "content": "You are expected to write a report to present your project findings such that another person could understand what are the tradeoffs between the two data systems you analyzed, and (perhaps more importantly) how you conducted your analysis. Your report therefore should include diagrams, code snippets, tables/visualizations (where appropriate), and broader reflections/recommendations about the usability of the systems themselves (ease of setup/debugging, etc.). You are required to submit both a checkpoint report (as part of the project checkpoint) and a final report. Please see the Deliverables section below for more details. We have included report templates in the project repository; you should feel free to start from these, and use your team’s preferred tool (e.g., Overleaf or Google Docs) to write the report. Report Components . Your report(s) should address the following. Items are tagged to indicate if they should be addressed in only the checkpoint report, only the final report, or both. | Dataset Selection. Explain your choice of dataset, and any concessions you needed to make. | both If you choose to use your own dataset, describe the data source and download process, and demonstrate that it meets the requirements laid out in the spec, and provide a link so that the staff can access the dataset. | checkpoint If it is a private dataset (e.g., IRB-restricted, personally identifiable information, etc.), you should not give dataset access to course staff. Instead, please send an email to data101@ and cc the instructors; we may ask to set up a short Zoom call with you. | . | both Decide whether to sample/truncate your dataset, and briefly explain your reasoning. If you choose to sample/truncate your dataset, describe how you did so. Do not include code here; you will do so in the later sections. | . | Database structure. | checkpoint A working database schema for the relational component of your dataset. This can be an ER diagram, or output from psql. | final For your Postgres system: . | An ER Diagram for your dataset, and | The complete database schema. For each table you create, share the result of executing \\d [table_name] in a psql console. | . | final Any additional adjustments you needed to make for your non-relational data system, including diagrams, snapshots, and code as needed. For example, if you chose MongoDB, you should list the collections that you created and the types of documents that you put in those collections. On the other hand, if you are using a pipeline tool, you should describe how you structured that tool. | . | System and Database Setup. | both Describe how you loaded data into Postgres, e.g. include code snippets, diagrams, etc. | checkpoint You do not need a full description, but you need to show enough progress that you have set up and imported a portion of the dataset. | both Are you using DataHub or your local computer(s) for compute resources? Are you using that same setup to store your database, or are you using another (cloud) system? | . | final Describe how you loaded data into your other system; include code snippets, diagrams, etc. | final Describe additional transformations you applied to your data. If you sampled, what techniques and tools did you use? What ways did you clean the data or address missing values? If your original data source was not in a rectangular format (e.g., JSON or XML), how did you translate it into a relational format? Did you have to make concessions in your database setup? For example, were you unable to translate certain parts of your dataset in either of the data systems? (You will have a chance to reflect on these tradeoffs later in your report.) Did you have to do any entity resolution across multiple parts of your dataset? | . | Task selection. | checkpoint An example of at least one query showing data from this dataset. You should explain what task the query is trying to accomplish, and why this task might be a reasonable approach for understanding your data system. For the checkpoint, this query does not need to be very complex. | final The full set of queries: 3 - 4 queries in SQL that demonstrate your dataset, and at least 2 queries of your non-relational database. For each query, you should: . | Explain what task the query is trying to accomplish | Explain why this is a reasonable tasks for understanding or evaluating your chosen data system | Include the code | (If useful for further explanation) include the output (or sample output if it is too large) | An analysis of its performance. What could make it faster? What did you try to make it faster? For example, if you added an index, show the before and after. | . | . | checkpoint Future plan. A plan for the non-relational parts of the project. A paragraph or two is fine. What other database do you plan to use? How might you plan to compare Postgres to this other database? | final Tool comparison. A comparison of tools (e.g., data systems) for fitness, ergonomics and performance. Consider the general use of the tool, installation, setup, etc. What was it like to learn the tool? What queries were “best” suited to that tool (performance, ease of writing, etc.) Does one tool allow tasks which are incredibly difficult in the other tool? | final, optional Additional tools used. If you used any additional tools beyond the two data systems, describe them here and why they were helpful (or not). | final Reflections. If someone else (e.g., your manager or labmate) were to consider these tools for similar tasks and/or workloads, what recommendations would you give them, and why? Think broadly beyond just the task/workload performance and consider the usability and ergonomics of the systems themselves: ease of setup, learning to use the tool, how useful it was to design and execute tasks, debugging capabilities, what worked well, what didn’t work so well, etc. | final Individual Reflections . | Each team member should include a paragraph about your personal reflections on the project. What did you learn? What did you find most exciting, or perhaps most frustrating? | It’s OK if you and your teammates share some of your reflections. | . | final, optional If relevant, include a reference page with citations of all outside sources used. | final, optional Appendix. Does not count towards your page limit, but could include more examples of your data, or example query output, or other sorts of visualizations and descriptions you may find useful to supplement the main report. | . Jump to Top . ",
    "url": "/sp25/assignments/final-project/#present-your-findings",
    
    "relUrl": "/assignments/final-project/#present-your-findings"
  },"52": {
    "doc": "Optional Final Project",
    "title": "Project Deliverables",
    "content": "1. Team Matching Survey (due TBD) . You must work in groups of three or four, and you must fill out the Team Matching Survey by TBD: . | If you don’t have a group, you must individually fill out the form and you will be randomly assigned a group. | If you have a partial group (e.g., a pair), one person must fill out the form by the deadline, and you will be randomly assigned 1-2 additional members. | If you do have a full group, one person must fill out the form by the deadline to declare your group. | Group confirmations will be sent out latest . | Please note that you will be evaluating your group members at the end of the project: we strongly encourage you to discuss and resolve any conflicts with your group members sooner rather than later. | . In very special circumstances (e.g., extenuating personal circumstances or an ongoing personal project such as a senior thesis), we will allow students to work alone. If you believe you qualify for this exception, please email data101@berkeley.edu AND cc instructors ASAP with relevant information/documentation. Do not assume the exception has been granted until you receive a confirmation email. 2. Checkpoint: Checkpoint Report and GitHub Repository (due TBD) . The checkpoint report is a relatively short report to make sure your team is on track. Your team will submit one GitHub Repository and a written report to Gradescope. See the Present Your Findings section for report components. Your checkpoint report is expected to be about 500-800 700-1600 words and should include code snippets, charts, or tables as necessary. You may choose to write your report in your team’s preferred tool (e.g., Overleaf or Google Docs). This file should be saved as 0-checkpoint.pdf and included in your GitHub repository. Your GitHub repository submission should include your report, and all code written to help produce the report. You should not include any raw dataset data in your GitHub repo. You will also be required to individually fill out a Google Form asking how the project is going in general. While we won’t share your comments to this Form directly with anyone else in your group, course staff will reach out to any groups that raise issues, so that we can help you resolve them and work productively with your group. You may use up to 2 slip days on the checkpoint submission. 3. Final Report and GitHub Repository (due TBD) . Your final report is expected to build upon much of your checkpoint report. Of course, some of your expectations from your checkpoint will likely need to be revised. Your team will submit one GitHub repository and written report to Gradescope. see the Present Your Findings section for report components. Your final report is expected to be about 1800-2500 2700-4000 words long and should include code snippets, charts, tables, and screenshots as necessary. You may choose to write your report in your team’s preferred tool (e.g., Overleaf or Google Docs). This file should be named 1-final-report.pdf and should be committed to your GitHub repo. Including diagrams and/or code, we expect your report to be 9–12 13-28 pages. We won’t be strictly enforcing these word/page limits, but reports that are much longer are subject to a penalty; reports that are much shorter are probably missing important discussion. Your GitHub repository submission should include your report, and all code written to help produce the report. You should not include any raw dataset data in your GitHub repo. You may use up to 2 slip days on the final report/repository submission. We will be releasing an outline template for the final report in the template repository on . 4. Team Member Assessment (due TBD) . This will be a short survey (submitted with your final submission) assessing you and your teammates contributions to the project. Each member will make an individual submission which is anonymous to your teammates. Jump to Top . ",
    "url": "/sp25/assignments/final-project/#project-deliverables",
    
    "relUrl": "/assignments/final-project/#project-deliverables"
  },"53": {
    "doc": "Optional Final Project",
    "title": "Grading",
    "content": "The final project is optional; if you submit it, it can count for up to 10% of your final grade. Note for Fall 2024, you will still need to take the final exam, so please take this into consideration when deciding whether to do this team project. The final project is worth 80 points: . | Initial Team Declaration Form [2 points] | Checkpoint [15 Points] | Final Report and Submission [60 points] . | Report Structure, Writing, and Content [15/60 points] | Tasks and Analysis [45/60 points] | . | Final Peer Assessment Form [3 points] | . A reasonable report would follow the provided scaffold in the template repository. A more detailed rubric will be provided closer to the checkpoint. Jump to Top . ",
    "url": "/sp25/assignments/final-project/#grading",
    
    "relUrl": "/assignments/final-project/#grading"
  },"54": {
    "doc": "Optional Final Project",
    "title": "Academic Integrity and Collaboration Policy",
    "content": "The final project is intentionally open-ended. As such there is no “correct answer”, which means you are encouraged to discuss and share approaches and tips with students in this class. However, you may not directly copy large amounts of written work or SQL queries that will be components of your final report. You must: . | Cite any sources when using code that was not generated by your or your teammates. | You should cite code in your GitHub repository using code comments. | . You may: . | Share code snippets on the relevant Ed threads, especially if they are related to tasks indirectly covered in class, like installation of tools, using psycopg, etc. | Use generative AI tools to ask high level questions, generate ideas and help debug code. | Use generative AI tools to help edit your writing, catch typos, improve clarity, etc. | . You must not: . | Directly use generative AI tools to write portions of your written report. | Copy entire queries from other students or online resources. | Outsource your analysis to anyone other than members of your team. | . Jump to Top . ",
    "url": "/sp25/assignments/final-project/#academic-integrity-and-collaboration-policy",
    
    "relUrl": "/assignments/final-project/#academic-integrity-and-collaboration-policy"
  },"55": {
    "doc": "Optional Final Project",
    "title": "Working in Groups",
    "content": "Here are some tips for working in your project groups. Usually, the best way to resolve any concerns with your group members is through proactive, direct, and considerate communication! . Most of the below tips are based on the collective wisdom of Data Science final project courses: . | Start a group chat and make sure everyone knows to check it regularly. | Consider meeting in person rather than online: students have told us that this tends to be more productive and efficient, and more fun too. | Don’t be afraid to reach out to course staff and come to office hours early if your group is struggling to set up systems or select/clean/load in data. You’ll likely be able to get more focused help at instructor content office hours. | Even if you divide up the work, it’s often helpful to have all group members contribute to brainstorming, reviewing results, and discussing how best to write up the report. | If you have concerns with your group members, communicate them earlier: don’t wait until the last minute! It’s much easier to resolve issues and improve collaboration early than the night before the deadline. | As part of the checkpoint, we’ll ask each individual how your project is going. While we won’t share your comments directly with anyone else in your group, course staff will reach out to any groups that raise issues, so that we can help you resolve them and work productively with your group. | . Jump to Top . ",
    "url": "/sp25/assignments/final-project/#working-in-groups",
    
    "relUrl": "/assignments/final-project/#working-in-groups"
  },"56": {
    "doc": "Optional Final Project",
    "title": "FAQ",
    "content": "Should I do the final project? Because we have a final exam for all students this semester, we recommend that you only do this optional final project if you have the capacity and motivation in this upcoming month to work in a team and produce a good report and solid results. If you are worried about your midterm grade, come talk to us. We’ll recommend some tips for studying for the final exam. If you would like to include this project on your resume, go for it. This project page (and the template repository) will be up past the end of the semester, so we recommend this as a reasonable personal project, say, over winter break, if that’s when you have more capacity. Can the final project lower my grade? No. We will compute the maximum of both grading options in the class. See the Syllabus for more information. Can I be on a team of less than 3 people? No. What if I decide not to do the final project? Please let us (via instructors, cc staff email data101@berkeley.edu) AND your group know ASAP. How do slip days work? Each person may use up to 2 slip days each on the checkpoint and the final report/repository submission. Slip days will be handled individually at the end of the semester. For example, if your team submits the checkpoint with 1 slip day and the final submission with 2 slip days, then each person gets a 3 slip day deduction from their total remaining slip days. This means that late penalties may be applied differently per team member. Can I make our team repository public after the end of the semester? We may reuse parts of this final project for next semester, so we prefer that you keep your project repositories private. However, you should definitely feel free to publicly share your team’s written report, provided that all your team members consent! One suggestion is to make an empty GitHub repository that holds your report PDF, with a README.md that tells people who come across your repository that code is available privately upon request. Can I get project help during office hours? We can offer general design and debugging tips during office hours, but we likely won't be able to help debug specific code. Instructors have opened up project office hours; please check the office hour calendar for more information. Jump to Top . ",
    "url": "/sp25/assignments/final-project/#faq",
    
    "relUrl": "/assignments/final-project/#faq"
  },"57": {
    "doc": "Optional Final Project",
    "title": "Acknowledgments",
    "content": "Select wording for the specifications (in particular, the entirety of “Working in Groups” and many components of the Grading Rubric) are drawn from the Data 102: Inference and Decision-Making (https://data102.org/) Final Project. ",
    "url": "/sp25/assignments/final-project/#acknowledgments",
    
    "relUrl": "/assignments/final-project/#acknowledgments"
  },"58": {
    "doc": "Home",
    "title": "Data 101 (Info 258): Data Engineering 💾",
    "content": " ",
    "url": "/sp25/#data-101-info-258-data-engineering-",
    
    "relUrl": "/#data-101-info-258-data-engineering-"
  },"59": {
    "doc": "Home",
    "title": "UC Berkeley, Spring 2025",
    "content": "Ed Lecture Recordings Gradescope Additional Extensions . Professor Aditya Parameswaran . He/Him . adityagp@berkeley.edu . ",
    "url": "/sp25/#uc-berkeley-spring-2025",
    
    "relUrl": "/#uc-berkeley-spring-2025"
  },"60": {
    "doc": "Home",
    "title": "Announcements",
    "content": "← Previous Week Next Week → ",
    "url": "/sp25/#announcements",
    
    "relUrl": "/#announcements"
  },"61": {
    "doc": "Home",
    "title": "Week 7 Announcement",
    "content": "Mar 3 Project 2 is due this Wednesday! Be sure to read the Midterm Logistics post as well. See Week 7 Ed announcement. ",
    "url": "/sp25/",
    
    "relUrl": "/"
  },"62": {
    "doc": "Home",
    "title": "Week 6 Announcement",
    "content": "Feb 24 Homework 2 is due this Wednesday! Project 2 is also released. See Week 6 Ed announcement. ",
    "url": "/sp25/",
    
    "relUrl": "/"
  },"63": {
    "doc": "Home",
    "title": "Week 5 Announcement",
    "content": "Feb 17 Project 1 is due this Wednesday! Homework 2 is also released. See Week 5 Ed announcement. ",
    "url": "/sp25/",
    
    "relUrl": "/"
  },"64": {
    "doc": "Home",
    "title": "Week 4 Announcement",
    "content": "Feb 10 Homework 1 is due this Wednesday! Project 1 is also released. See Week 5 Ed announcement. ",
    "url": "/sp25/",
    
    "relUrl": "/"
  },"65": {
    "doc": "Home",
    "title": "Week 3 Announcement",
    "content": "Feb 3 Project 0 is due this Friday and Homework 1 is released! . See Week 3 Ed announcement. ",
    "url": "/sp25/",
    
    "relUrl": "/"
  },"66": {
    "doc": "Home",
    "title": "Week 2 Announcement",
    "content": "Jan 27 . | Welcome to back to the second week of classes! See Week 02 Announcements on Ed. | Office hours start this week at Warren 101B! | We’ve released the Data Scholars Interest Form | Project 0 is due Friday 2/7 at 5pm. | Homework 1 will be released on Gradescope on Friday 1/31 and is due on Wednesday 2/12 at 5pm. | Lecture attendance starts this week! | Discussion sections will start on Thursday and Friday. You can find specific hours on the calendar. | . ",
    "url": "/sp25/",
    
    "relUrl": "/"
  },"67": {
    "doc": "Home",
    "title": "Week 1 Announcement",
    "content": "Jan 20 . | Welcome to Data 101! | Sections start next Thursday 1/30 and Friday 1/31. Join us for a review of SQL, and meet some study partners! . | Sections times/locations will be updated on the calendar by the end of the weekend calendar | . | Office hours start next week! You can find the times and locations on the calendar. | Please make sure to check the “Welcome to Data C101 / Info 258!” post on Ed! This details the Pre-Semester survey and other logistics. | Check out our cool new logo (top left) designed by one of our TA’s, Li! | . ",
    "url": "/sp25/",
    
    "relUrl": "/"
  },"68": {
    "doc": "Home",
    "title": "Week 0 Announcement",
    "content": "Jan 18 Welcome to data engineering! Check back on this site for more updates . ",
    "url": "/sp25/",
    
    "relUrl": "/"
  },"69": {
    "doc": "Home",
    "title": "Schedule",
    "content": " ",
    "url": "/sp25/#schedule",
    
    "relUrl": "/#schedule"
  },"70": {
    "doc": "Home",
    "title": "Week 1",
    "content": "Tue 1/21 Lecture 1 Introduction, Data Engineering Lifecycle Pre-Semester Form Thu 1/23 Lecture 2 SQL Review Course Notes Fri 1/24 Project 0 SQL Review Due Fri 2/7, 5pm ",
    "url": "/sp25/#week-1",
    
    "relUrl": "/#week-1"
  },"71": {
    "doc": "Home",
    "title": "Week 2",
    "content": "Tue 1/28 Lecture 3 SQL Review (continued), Relational Model &amp; Algebra Course Notes Thu 1/30 Lecture 4 Relational Algebra (continued), CTEs, Views, Subqueries Course Notes Discussion 1 SQL Review Code, Solution Fri 1/31 Homework 1 Homework 1 Due Wed 2/12, 5pm ",
    "url": "/sp25/#week-2",
    
    "relUrl": "/#week-2"
  },"72": {
    "doc": "Home",
    "title": "Week 3",
    "content": "Tue 2/4 Lecture 5 DML, DDL, Referential Integrity, Constraints Course Notes Thu 2/6 Lecture 6 Bits/memory model, Performance Tuning, Index Selection Course Notes Discussion 2 Relational Algebra, Subqueries, CTEs, Joins Solution Fri 2/7 Project 0 Due, 5pm Project 1 SQL Due Wed 2/19, 5pm ",
    "url": "/sp25/#week-3",
    
    "relUrl": "/#week-3"
  },"73": {
    "doc": "Home",
    "title": "Week 4",
    "content": "Tue 2/11 Lecture 7 Optimizing for Performance I Course Notes Wed 2/12 Homework 1 Due, 5pm Thu 2/13 Lecture 8 Optimizing for Performance II Course Notes Discussion 3 DML/DDL, Bits Conversion, Query Performance Solution, Code Fri 2/14 Homework 2 Homework 2 Due Wed 2/26, 5pm ",
    "url": "/sp25/#week-4",
    
    "relUrl": "/#week-4"
  },"74": {
    "doc": "Home",
    "title": "Week 5",
    "content": "Tue 2/18 Lecture 9 Optimizing for Performance III Course Notes Wed 2/19 Project 1 Due, 5pm Thu 2/20 Lecture 10 Data Modeling I Course Notes Discussion 4 Query Performance Solution, Code Fri 2/21 Project 2 Query Performance Due Wed 3/5, 5pm ",
    "url": "/sp25/#week-5",
    
    "relUrl": "/#week-5"
  },"75": {
    "doc": "Home",
    "title": "Week 6",
    "content": "Tue 2/25 Lecture 11 Data Preparation I: Structural Course Notes Wed 2/26 Homework 2 Due, 5pm Thu 2/27 Lecture 12 Data Preparation II: Numerical, Granularity, Window Functions Course Notes Discussion 5 Data Models, Data Preparation Solution, Code Fri 2/28 Homework 3 Homework 3 Due Wed 3/19, 5pm ",
    "url": "/sp25/#week-6",
    
    "relUrl": "/#week-6"
  },"76": {
    "doc": "Home",
    "title": "Week 7",
    "content": "Tue 3/4 Lecture 13 Data Preparation III: Outliers Course Notes Wed 3/5 Project 2 Due, 5pm Thu 3/6 Lecture 14 Data Preparation IV: Imputation, Entity Resolution Course Notes Discussion 6 TBA Solution ",
    "url": "/sp25/#week-7",
    
    "relUrl": "/#week-7"
  },"77": {
    "doc": "Home",
    "title": "Week 8",
    "content": "Tue 3/11 Lecture 15 Data Modeling II: Normalization + ER Mid-Semester Form Wed 3/12 Midterm Midterm Exam (6-8pm) Thu 3/13 Lecture 16 No Class Discussion No Discussion ",
    "url": "/sp25/#week-8",
    
    "relUrl": "/#week-8"
  },"78": {
    "doc": "Home",
    "title": "Week 9",
    "content": "Tue 3/18 Lecture 17 Semistructured Data: NoSQL, JSON, XML Wed 3/19 Homework 3 Due, 5pm Thu 3/20 Lecture 18 MongoDB I Discussion 7 TBA Solution Fri 3/21 Project 3 Data Transformation Due Fri 4/4, 5pm ",
    "url": "/sp25/#week-9",
    
    "relUrl": "/#week-9"
  },"79": {
    "doc": "Home",
    "title": "Week 10",
    "content": "All Week Spring Break ",
    "url": "/sp25/#week-10",
    
    "relUrl": "/#week-10"
  },"80": {
    "doc": "Home",
    "title": "Week 11",
    "content": "Mon 3/31 Homework 4 Homework 4 Due Wed 4/9, 5pm Tue 4/1 Lecture 19 MongoDB II Thu 4/3 Lecture 20 Data Ops and Pipelines Discussion 8 TBA Solution Fri 4/4 Project 3 Due, 5pm Project 4 Mongo Due Wed 4/16, 5pm Project 5 Optional* Final Project Checkpoint due Mon 4/21, 5pm Final Report due Fri 5/2, 5pm ",
    "url": "/sp25/#week-11",
    
    "relUrl": "/#week-11"
  },"81": {
    "doc": "Home",
    "title": "Week 12",
    "content": "Tue 4/5 Lecture 21 MapReduce, Sampling Wed 4/6 Homework 4 Due, 5pm Thu 4/7 Lecture 22 Transactions and TCL Discussion 9 TBA Solution ",
    "url": "/sp25/#week-12",
    
    "relUrl": "/#week-12"
  },"82": {
    "doc": "Home",
    "title": "Week 13",
    "content": "Tue 4/15 Lecture 23 Transactions, BI, OLAP Wed 4/16 Project 4 Due, 5pm Thu 4/17 Lecture 24 Spreadsheets Fri 4/18 Homework 5 Homework 5 Due Wed 4/30, 5pm ",
    "url": "/sp25/#week-13",
    
    "relUrl": "/#week-13"
  },"83": {
    "doc": "Home",
    "title": "Week 14",
    "content": "Mon 4/21 Project 5 Checkpoint due, 5pm Tue 4/22 Lecture 25 Parallel and Distributed Computing, CAP Theorem, VMs Thu 4/24 Lecture 26 Graphs Databases and Knowledge Bases Discussion 10 TBA Solution ",
    "url": "/sp25/#week-14",
    
    "relUrl": "/#week-14"
  },"84": {
    "doc": "Home",
    "title": "Week 15",
    "content": "Tue 4/29 Lecture 27 TBA End-of-Semester Form Wed 4/30 Homework 5 Due, 5pm Thu 5/1 Lecture 28 Guest Lecture, Closing Thoughts Fri 5/2 Project 5 Final Report due, 5pm ",
    "url": "/sp25/#week-15",
    
    "relUrl": "/#week-15"
  },"85": {
    "doc": "Home",
    "title": "RRR Week",
    "content": "All Week RRR Week ",
    "url": "/sp25/#rrr-week",
    
    "relUrl": "/#rrr-week"
  },"86": {
    "doc": "Home",
    "title": "Finals Week",
    "content": "Wed 5/14 Final Final Exam (11:30am-2:30pm) ",
    "url": "/sp25/#finals-week",
    
    "relUrl": "/#finals-week"
  },"87": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/sp25/",
    
    "relUrl": "/"
  },"88": {
    "doc": "Lecture 18",
    "title": "Lecture 18: Mongo DB II¶",
    "content": "In [1]: import json import pymongo import pprint . In [2]: # instead of saving a full copy of existing data, # just symlink to previous lecture. a bit hacky, but saves space! !ln -sf ../lec17-mongo_i/data data . In [2]: # this is a utility function we define so that find_all() prints nicely def pretty_print(output_collection, n_to_print=3): \"\"\" note if n_to_print is -1, this print everything \"\"\" for i, doc in enumerate(output_collection): pprint.pprint(doc) # nicely formats each document if i+1 == n_to_print: return . In [ ]: . ",
    "url": "/sp25/assets/lectures/lec18/#Lecture-18:-Mongo-DB-II",
    
    "relUrl": "/assets/lectures/lec18/#Lecture-18:-Mongo-DB-II"
  },"89": {
    "doc": "Lecture 18",
    "title": "Aggregation Queries¶",
    "content": "Zips JSON from the MongoDB Aggregation Tutorial: https://www.mongodb.com/docs/manual/tutorial/aggregation-zip-code-data-set/ . ",
    "url": "/sp25/assets/lectures/lec18/#Aggregation-Queries",
    
    "relUrl": "/assets/lectures/lec18/#Aggregation-Queries"
  },"90": {
    "doc": "Lecture 18",
    "title": "Load zips.json into new local.zips¶",
    "content": "For the sake of simplicity, we'll make a new collection zips in a new aggquerydb database. In [3]: # reimport/redefine as needed import json import pymongo import pprint def pretty_print(output_collection, n_to_print=3): \"\"\" note if n_to_print is -1, this print everything \"\"\" for i, doc in enumerate(output_collection): pprint.pprint(doc) # nicely formats each document if i+1 == n_to_print: return . In [5]: client = pymongo.MongoClient('mongodb://localhost') client.list_database_names() . Out[5]: ['admin', 'config', 'local', 'nobel_prizes'] . In [6]: # run this cell to make the new collection # and insert zipcode documents client.drop_database('aggquerydb') # if already exists db = client.aggquerydb with open('data/zips.json', encoding='utf-8') as f: for line in f.readlines(): db.zips.insert_one(json.loads(line)) . In [7]: db.zips.count_documents({}) . Out[7]: 29353 . ",
    "url": "/sp25/assets/lectures/lec18/#Load-zips.json-into-new-local.zips",
    
    "relUrl": "/assets/lectures/lec18/#Load-zips.json-into-new-local.zips"
  },"91": {
    "doc": "Lecture 18",
    "title": "$group¶",
    "content": "In [7]: # Just the grouping, no filtering output = db.zips.aggregate( [ { \"$group\": { \"_id\": \"$state\", \"totalPop\": { \"$sum\": \"$pop\" } } }, ] ) pretty_print(output, n_to_print=5) . {'_id': 'ND', 'totalPop': 638272} {'_id': 'AR', 'totalPop': 2350725} {'_id': 'HI', 'totalPop': 1108229} {'_id': 'WV', 'totalPop': 1793146} {'_id': 'TX', 'totalPop': 16984601} . In [9]: output = db.zips.aggregate( [ { \"$group\": { \"_id\": \"$state\", \"totalPop\": { \"$sum\": \"$pop\" } } }, { \"$match\": { \"totalPop\": { \"$gte\": 15000000 } } }, { \"$sort\" : { \"totalPop\" : -1 } } ] ) pretty_print(output, n_to_print=10) . {'_id': 'CA', 'totalPop': 29754890} {'_id': 'NY', 'totalPop': 17990402} {'_id': 'TX', 'totalPop': 16984601} . ",
    "url": "/sp25/assets/lectures/lec18/#$group",
    
    "relUrl": "/assets/lectures/lec18/#$group"
  },"92": {
    "doc": "Lecture 18",
    "title": "Aggregation Queries: Unwind/Lookup¶",
    "content": "Make a new collection, inventory. In [10]: db = client.aggquerydb # stay in same database db.drop_collection('inventory') # recreate as needed db.inventory.insert_many( [ { \"item\": \"journal\", \"tags\": [\"blank\", \"red\"], \"dim\": [ 14, 21 ], \"instock\": [ { \"loc\": \"A\", \"qty\": 5 }, { \"loc\": \"C\", \"qty\": 15 } ] }, { \"item\": \"notebook\", \"tags\": [\"red\", \"blank\"], \"dim\": [ 14, 21 ], \"instock\": [ { \"loc\": \"C\", \"qty\": 5 } ] }, { \"item\": \"paper\", \"tags\": [\"red\", \"blank\", \"plain\"], \"dim\": [ 14, 21 ], \"instock\": [ { \"loc\": \"A\", \"qty\": 60 }, { \"loc\": \"B\", \"qty\": 15 } ] }, { \"item\": \"planner\", \"tags\": [\"blank\", \"red\"], \"dim\": [ 22.85, 30 ] , \"instock\": [ { \"loc\": \"A\", \"qty\": 40 }, { \"loc\": \"B\", \"qty\": 5 } ] }, { \"item\": \"postcard\", \"tags\": [\"blue\"], \"dim\": [ 10, 15.25 ], \"instock\": [ { \"loc\": \"E\", \"qty\": 15 }, { \"loc\": \"D\", \"qty\": 35 } ] } ]); . In [ ]: . In [11]: pretty_print(db.inventory.find({})) . {'_id': ObjectId('6723bcb2c96a1d9cd406ab20'), 'dim': [14, 21], 'instock': [{'loc': 'A', 'qty': 5}, {'loc': 'C', 'qty': 15}], 'item': 'journal', 'tags': ['blank', 'red']} {'_id': ObjectId('6723bcb2c96a1d9cd406ab21'), 'dim': [14, 21], 'instock': [{'loc': 'C', 'qty': 5}], 'item': 'notebook', 'tags': ['red', 'blank']} {'_id': ObjectId('6723bcb2c96a1d9cd406ab22'), 'dim': [14, 21], 'instock': [{'loc': 'A', 'qty': 60}, {'loc': 'B', 'qty': 15}], 'item': 'paper', 'tags': ['red', 'blank', 'plain']} . ",
    "url": "/sp25/assets/lectures/lec18/#Aggregation-Queries:-Unwind/Lookup",
    
    "relUrl": "/assets/lectures/lec18/#Aggregation-Queries:-Unwind/Lookup"
  },"93": {
    "doc": "Lecture 18",
    "title": "Unwind example¶",
    "content": "In [12]: # Notice, tags is no longer an array output = db.inventory.aggregate( [ { \"$unwind\" : \"$tags\" } ] ) pretty_print(output, n_to_print=4) . {'_id': ObjectId('6723bcb2c96a1d9cd406ab20'), 'dim': [14, 21], 'instock': [{'loc': 'A', 'qty': 5}, {'loc': 'C', 'qty': 15}], 'item': 'journal', 'tags': 'blank'} {'_id': ObjectId('6723bcb2c96a1d9cd406ab20'), 'dim': [14, 21], 'instock': [{'loc': 'A', 'qty': 5}, {'loc': 'C', 'qty': 15}], 'item': 'journal', 'tags': 'red'} {'_id': ObjectId('6723bcb2c96a1d9cd406ab21'), 'dim': [14, 21], 'instock': [{'loc': 'C', 'qty': 5}], 'item': 'notebook', 'tags': 'red'} {'_id': ObjectId('6723bcb2c96a1d9cd406ab21'), 'dim': [14, 21], 'instock': [{'loc': 'C', 'qty': 5}], 'item': 'notebook', 'tags': 'blank'} . In [13]: output = db.inventory.aggregate( [ { \"$unwind\" : \"$tags\" }, { \"$project\" : {\"_id\" : 0, \"instock\": 0}} ] ) pretty_print(output, n_to_print=-1) . {'dim': [14, 21], 'item': 'journal', 'tags': 'blank'} {'dim': [14, 21], 'item': 'journal', 'tags': 'red'} {'dim': [14, 21], 'item': 'notebook', 'tags': 'red'} {'dim': [14, 21], 'item': 'notebook', 'tags': 'blank'} {'dim': [14, 21], 'item': 'paper', 'tags': 'red'} {'dim': [14, 21], 'item': 'paper', 'tags': 'blank'} {'dim': [14, 21], 'item': 'paper', 'tags': 'plain'} {'dim': [22.85, 30], 'item': 'planner', 'tags': 'blank'} {'dim': [22.85, 30], 'item': 'planner', 'tags': 'red'} {'dim': [10, 15.25], 'item': 'postcard', 'tags': 'blue'} . In [14]: output = db.inventory.aggregate( [ { \"$unwind\" : \"$instock\" }, { \"$group\" : { \"_id\" : \"$item\", \"totalqty\" : {\"$sum\" : \"$instock.qty\"}}} ] ) pretty_print(output, n_to_print=-1) . {'_id': 'planner', 'totalqty': 45} {'_id': 'journal', 'totalqty': 20} {'_id': 'postcard', 'totalqty': 50} {'_id': 'paper', 'totalqty': 75} {'_id': 'notebook', 'totalqty': 5} . In [16]: output = db.inventory.aggregate( [ { \"$lookup\" : { \"from\" : \"inventory\", \"localField\": \"instock.loc\", \"foreignField\": \"instock.loc\", \"as\":\"otheritems\"} }, { \"$project\" : {\"_id\" : 0, \"tags\" : 0, \"dim\" : 0, \"otheritems._id\": 0} } ] ) pretty_print(output, n_to_print=2) . {'instock': [{'loc': 'A', 'qty': 5}, {'loc': 'C', 'qty': 15}], 'item': 'journal', 'otheritems': [{'dim': [14, 21], 'instock': [{'loc': 'A', 'qty': 5}, {'loc': 'C', 'qty': 15}], 'item': 'journal', 'tags': ['blank', 'red']}, {'dim': [14, 21], 'instock': [{'loc': 'C', 'qty': 5}], 'item': 'notebook', 'tags': ['red', 'blank']}, {'dim': [14, 21], 'instock': [{'loc': 'A', 'qty': 60}, {'loc': 'B', 'qty': 15}], 'item': 'paper', 'tags': ['red', 'blank', 'plain']}, {'dim': [22.85, 30], 'instock': [{'loc': 'A', 'qty': 40}, {'loc': 'B', 'qty': 5}], 'item': 'planner', 'tags': ['blank', 'red']}]} {'instock': [{'loc': 'C', 'qty': 5}], 'item': 'notebook', 'otheritems': [{'dim': [14, 21], 'instock': [{'loc': 'A', 'qty': 5}, {'loc': 'C', 'qty': 15}], 'item': 'journal', 'tags': ['blank', 'red']}, {'dim': [14, 21], 'instock': [{'loc': 'C', 'qty': 5}], 'item': 'notebook', 'tags': ['red', 'blank']}]} . ",
    "url": "/sp25/assets/lectures/lec18/#Unwind-example",
    
    "relUrl": "/assets/lectures/lec18/#Unwind-example"
  },"94": {
    "doc": "Lecture 18",
    "title": "Multiple Attribute Grouping¶",
    "content": "In [17]: # reimport/redefine as needed import json import pymongo import pprint def pretty_print(output_collection, n_to_print=3): \"\"\" note if n_to_print is -1, this print everything \"\"\" for i, doc in enumerate(output_collection): pprint.pprint(doc) # nicely formats each document if i+1 == n_to_print: return . In [19]: client = pymongo.MongoClient('mongodb://localhost') client.list_database_names() . Out[19]: ['admin', 'aggquerydb', 'config', 'local', 'nobel_prizes'] . In [20]: db = client.aggquerydb . In [24]: db.zips.count_documents({}) . Out[24]: 29353 . In [18]: db.zips.find_one() . Out[18]: {'_id': '01001', 'city': 'AGAWAM', 'loc': [-72.622739, 42.070206], 'pop': 15338, 'state': 'MA'} . | What is this doing? | . In [19]: # Break things down by steps if necessary. output = db.zips.aggregate( [ { \"$group\": { \"_id\": { \"state\": \"$state\", \"city\": \"$city\" }, \"pop\": { \"$sum\": \"$pop\" } } } ] ) pretty_print(output, n_to_print=10) . {'_id': {'city': 'VALMEYER', 'state': 'IL'}, 'pop': 1841} {'_id': {'city': 'PIEDMONT', 'state': 'MO'}, 'pop': 3719} {'_id': {'city': 'HOPKINSVILLE', 'state': 'KY'}, 'pop': 39331} {'_id': {'city': 'MINIER', 'state': 'IL'}, 'pop': 1483} {'_id': {'city': 'PARAMUS', 'state': 'NJ'}, 'pop': 25085} {'_id': {'city': 'SEVERNA PARK', 'state': 'MD'}, 'pop': 23392} {'_id': {'city': 'HUNTINGTON', 'state': 'AR'}, 'pop': 2608} {'_id': {'city': 'LUTHER', 'state': 'OK'}, 'pop': 2111} {'_id': {'city': 'STATE CENTER', 'state': 'IA'}, 'pop': 1618} {'_id': {'city': 'TAFT', 'state': 'OK'}, 'pop': 1822} . In [20]: output = db.zips.aggregate( [ { \"$group\": { \"_id\": { \"state\": \"$state\", \"city\": \"$city\" }, \"pop\": { \"$sum\": \"$pop\" } } }, { \"$group\": { \"_id\": \"$_id.state\", \"avgCityPop\": { \"$avg\": \"$pop\" } } } ] ) pretty_print(output, n_to_print=10) . {'_id': 'NY', 'avgCityPop': 13131.680291970803} {'_id': 'MT', 'avgCityPop': 2593.987012987013} {'_id': 'MO', 'avgCityPop': 5672.195338512764} {'_id': 'CO', 'avgCityPop': 9981.075757575758} {'_id': 'CA', 'avgCityPop': 27756.42723880597} {'_id': 'KY', 'avgCityPop': 4767.164721141375} {'_id': 'OK', 'avgCityPop': 6155.743639921722} {'_id': 'NV', 'avgCityPop': 18209.590909090908} {'_id': 'MD', 'avgCityPop': 12615.775725593667} {'_id': 'DC', 'avgCityPop': 303450.0} . | What is this doing? | . In [25]: output = db.zips.aggregate( [ { \"$group\": { \"_id\": { \"state\": \"$state\", \"city\": \"$city\" }, \"pop\": { \"$sum\": \"$pop\" } } }, { \"$sort\": { \"pop\": -1 } }, { \"$group\": { \"_id\" : \"$_id.state\", \"bigCity\": { \"$first\": \"$_id.city\" }, \"bigPop\": { \"$first\": \"$pop\" } } }, { \"$sort\" : {\"bigPop\" : -1} }, { \"$project\" : {\"bigPop\" : 0} } ] ) pretty_print(output, n_to_print=10) . {'_id': 'IL', 'bigCity': 'CHICAGO'} {'_id': 'NY', 'bigCity': 'BROOKLYN'} {'_id': 'CA', 'bigCity': 'LOS ANGELES'} {'_id': 'TX', 'bigCity': 'HOUSTON'} {'_id': 'PA', 'bigCity': 'PHILADELPHIA'} {'_id': 'MI', 'bigCity': 'DETROIT'} {'_id': 'AZ', 'bigCity': 'PHOENIX'} {'_id': 'FL', 'bigCity': 'MIAMI'} {'_id': 'MD', 'bigCity': 'BALTIMORE'} {'_id': 'TN', 'bigCity': 'MEMPHIS'} . In [ ]: . | What is this doing? | . In [27]: output = db.zips.aggregate( [ { \"$group\": { \"_id\": { \"state\": \"$state\", \"city\": \"$city\" }, \"pop\": { \"$sum\": \"$pop\" } } }, { \"$sort\": { \"pop\": -1 } }, { \"$group\": { \"_id\" : \"$_id.state\", \"bigCity\": { \"$first\": \"$_id.city\" }, \"bigPop\": { \"$first\": \"$pop\" } } }, { \"$sort\" : {\"bigPop\" : -1} }, { \"$project\" : { \"_id\" : 0, \"state\" : \"$_id\", \"bigCityDeets\": { \"name\": \"$bigCity\", \"pop\": \"$bigPop\" } } } ] ) pretty_print(output, n_to_print=10) . {'bigCityDeets': {'name': 'CHICAGO', 'pop': 2452177}, 'state': 'IL'} {'bigCityDeets': {'name': 'BROOKLYN', 'pop': 2300504}, 'state': 'NY'} {'bigCityDeets': {'name': 'LOS ANGELES', 'pop': 2102295}, 'state': 'CA'} {'bigCityDeets': {'name': 'HOUSTON', 'pop': 2095918}, 'state': 'TX'} {'bigCityDeets': {'name': 'PHILADELPHIA', 'pop': 1610956}, 'state': 'PA'} {'bigCityDeets': {'name': 'DETROIT', 'pop': 963243}, 'state': 'MI'} {'bigCityDeets': {'name': 'PHOENIX', 'pop': 890853}, 'state': 'AZ'} {'bigCityDeets': {'name': 'MIAMI', 'pop': 825232}, 'state': 'FL'} {'bigCityDeets': {'name': 'BALTIMORE', 'pop': 733081}, 'state': 'MD'} {'bigCityDeets': {'name': 'MEMPHIS', 'pop': 632837}, 'state': 'TN'} . In [ ]: . ",
    "url": "/sp25/assets/lectures/lec18/#Multiple-Attribute-Grouping",
    
    "relUrl": "/assets/lectures/lec18/#Multiple-Attribute-Grouping"
  },"95": {
    "doc": "Lecture 18",
    "title": "[Extra] Aggregation Pipeline Demos¶",
    "content": "Nobel laureate data from Kaggle: https://www.kaggle.com/datasets/imdevskp/nobel-prize/data . In [30]: client = pymongo.MongoClient('mongodb://localhost') client.list_database_names() client.drop_database('nobel_prizes') # if already exists db = client.nobel_prizes # run this cell to insert into the collection prizes with open('data/prize.json', encoding='utf-8') as f: db.prizes.insert_many(json.loads(f.read())) print(\"databases\", client.list_database_names()) db = client.nobel_prizes print(\"collections in nobel_prizes database\", db.list_collection_names()) . databases ['admin', 'aggquerydb', 'config', 'local', 'nobel_prizes'] collections in nobel_prizes database ['prizes'] . In [ ]: . In [ ]: # A output = db.prizes.aggregate([{\"$group\": {\"_id\": \"$category\", \"awardyears\": {\"$sum\" : 1}}}]) pretty_print(output, n_to_print=-1) . In [ ]: # B output = db.prizes.aggregate([{\"$group\": {\"_id\": \"$category\", \"awardyears\": {\"$sum\" : 1}}}, {\"$match\" : {\"awardyears\": {\"$lt\": 100}}}]) pretty_print(output, n_to_print=-1) . In [ ]: # C output = db.prizes.aggregate([{\"$group\": {\"_id\": \"$category\", \"awardyears\": {\"$sum\" : 1}}}, {\"$match\" : {\"awardyears\": {\"$lt\": 100}}}, {\"$project\" : {\"_id\": 0, \"awardyears\": 1}}]) pretty_print(output, n_to_print=-1) . In [ ]: # D output = db.prizes.aggregate([{\"$unwind\": \"$laureates\"}, {\"$group\": {\"_id\": \"$category\", \"awards\": {\"$sum\" : 1}}}]) pretty_print(output, n_to_print=-1) . In [ ]: # E output = db.prizes.aggregate([{\"$unwind\": \"$laureates\"}, {\"$group\": {\"_id\": {\"category\": \"$category\", \"year\": \"$year\"}, \"awards\": {\"$sum\" : 1}}}]) pretty_print(output, n_to_print=10) . In [ ]: # F output = db.prizes.aggregate([{\"$unwind\": \"$laureates\"}, {\"$group\": {\"_id\": {\"category\": \"$category\", \"year\": \"$year\"}, \"awards\": {\"$sum\" : 1}}}, {\"$sort\" : {\"awards\": -1}}]) pretty_print(output, n_to_print=15) . In [ ]: # G output = db.prizes.aggregate([{\"$unwind\": \"$laureates\"}, {\"$group\": {\"_id\": {\"category\": \"$category\", \"year\": \"$year\"}, \"awards\": {\"$sum\" : 1}}}, {\"$group\": {\"_id\":\"$_id.category\", \"avgawards\": {\"$avg\" : \"$awards\"}}}]) pretty_print(output, n_to_print=-1) . ",
    "url": "/sp25/assets/lectures/lec18/#%5BExtra%5D-Aggregation-Pipeline-Demos",
    
    "relUrl": "/assets/lectures/lec18/#%5BExtra%5D-Aggregation-Pipeline-Demos"
  },"96": {
    "doc": "Lecture 18",
    "title": "Lecture 18",
    "content": " ",
    "url": "/sp25/assets/lectures/lec18/",
    
    "relUrl": "/assets/lectures/lec18/"
  },"97": {
    "doc": "Resources",
    "title": "Resources",
    "content": "Jump to: . | Resources . | Prerequisites | Course Notes | SQL | Assignment Tips | Exam Resources | Wellness Resources . | Device Lending Options | . | . | . ",
    "url": "/sp25/resources/",
    
    "relUrl": "/resources/"
  },"98": {
    "doc": "Resources",
    "title": "Prerequisites",
    "content": "The Data 100 textbook, the CS 61B curriculum, and the CS 61A textbook are excellent resources for brushing up on material from the prerequisites. ",
    "url": "/sp25/resources/#prerequisites",
    
    "relUrl": "/resources/#prerequisites"
  },"99": {
    "doc": "Resources",
    "title": "Course Notes",
    "content": "The Data 101 course notes (found on the main page) are a work in progress and are meant to be a resource for you to refer to as you complete assignments. However, they are not a perfect substitute for lectures, as they are a passive resource—meaning you may read something but not understand it the first time. Lectures will have participation and engagement activities to help you practice the material. Please attend! . ",
    "url": "/sp25/resources/#course-notes",
    
    "relUrl": "/resources/#course-notes"
  },"100": {
    "doc": "Resources",
    "title": "SQL",
    "content": "The PostgresSQL documentation can be read front to back. Highly recommended! . Other resources from Data 100, which are intended for SQLite: . | We’ve assembled some SQL Review Slides to help you brush up on SQL. | We’ve also compiled a list of SQL practice problems, which can be found here, along with their solutions. | This SQL Cheat Sheet is an awesome resource that was created by Luke Harrison, a former Data 100 student. | . ",
    "url": "/sp25/resources/#sql",
    
    "relUrl": "/resources/#sql"
  },"101": {
    "doc": "Resources",
    "title": "Assignment Tips",
    "content": "See Assignment Tips. ",
    "url": "/sp25/resources/#assignment-tips",
    
    "relUrl": "/resources/#assignment-tips"
  },"102": {
    "doc": "Resources",
    "title": "Exam Resources",
    "content": "| Semester | Midterm | Final | Reference Packet | Notes | . | Fall 2024 | Exam (Solutions) | Exam (Solutions) | Midterm, Final | Common Errors: Midterm, Final | . | Fall 2023 |   | Exam (Solutions) | Final | Common Errors | . | Fall 2022 |   | Exam (Solutions) |   | Shortened exam due to the UAW 2865 ASE strike | . Note: Reference sheets/packets will be provided with the paper exam; you should not need to print your own copy. Please post to Ed if you find typos. Thanks! . The best way to get a sense for what is needed for our written exam is to attend discussion throughout the semester and work on discussion handouts. ",
    "url": "/sp25/resources/#exam-resources",
    
    "relUrl": "/resources/#exam-resources"
  },"103": {
    "doc": "Resources",
    "title": "Wellness Resources",
    "content": "Your well-being matters, and we hope that Data 100 is never a barrier to taking care of your mental and physical health. Below are some campus resources that may be helpful. COVID-19 Resources and Support . You can find UC Berkeley’ COVID-19 resources and support here. For academic performance, support, and technology . The Center for Access to Engineering Excellence (Bechtel Engineering Center 227) is an inclusive center that offers study spaces, nutritious snacks, and tutoring in &gt;50 courses for Berkeley engineers and other majors across campus. The Center also offers a wide range of professional development, leadership, and wellness programs, and loans iclickers, laptops, and professional attire for interviews. As the primary academic support service for undergraduates at UC Berkeley, the Student Learning Center (510-642-7332) assists students in transitioning to Cal, navigating the academic terrain, creating networks of resources, and achieving academic, personal, and professional goals. Through various services including tutoring, study groups, workshops, and courses, SLC supports undergraduate students in Biological and Physical Sciences, Business Administration, Computer Science, Economics, Mathematics, Social Sciences, Statistics, Study Strategies, and Writing. The Educational Opportunity Program (EOP, Cesar Chavez Student Center 119; 510-642-7224) at Cal has provided first generation and low income college students with the guidance and resources necessary to succeed at the best public university in the world. EOP’s individualized academic counseling, support services, and extensive campus referral network help students develop the unique gifts and talents they each bring to the university while empowering them to achieve. Device Lending Options . Students can access device lending options through the Student Technology Equity Program (STEP) program. For mental well-being . The staff of the UHS Counseling and Psychological Services (Tang Center, 2222 Bancroft Way; 510-642-9494; for after-hours support, please call the 24/7 line at 855-817-5667) provides confidential, brief counseling and crisis intervention to students with personal, academic and career stress. Services are provided by a multicultural group of professional counselors including psychologists, social workers, and advanced level trainees. All undergraduate and graduate students are eligible for CAPS services, regardless of insurance coverage. To improve access for engineering students, a licensed psychologist from the Tang Center also holds walk-in appointments for confidential counseling in Bechtel Engineering Center 241 (be sure to check the schedule). For disability accommodations . The Disabled Students’ Program (DSP, 260 César Chávez Student Center #4250; 510-642-0518) serves students with disabilities of all kinds, including mobility impairments, blind or low vision, deaf or hard of hearing; chronic illnesses (chronic pain, repetitive strain injuries, brain injuries, AIDS/HIV, cancer, etc.) psychological disabilities (bipolar disorder, severe anxiety or depression, etc.), Attention Deficit Disorder/Attention Deficit Hyperactivity Disorder, and Learning Disabilities. Services are individually designed and based on the specific needs of each student as identified by DSP’s Specialists. The Program’s official website includes information on DSP staff, UCB’s disabilities policy, application procedures, campus access guides for most university buildings, and portals for students and faculty. For solving a dispute . The Ombudsperson for Students (Sproul Hall 102; 510-642-5754) provides a confidential service for students involved in a University-related problem (academic or administrative), acting as a neutral complaint resolver and not as an advocate for any of the parties involved in a dispute. The Ombudsperson can provide information on policies and procedures affecting students, facilitate students’ contact with services able to assist in resolving the problem, and assist students in complaints concerning improper application of University policies or procedures. All matters referred to this office are held in strict confidence. The only exceptions, at the sole discretion of the Ombudsperson, are cases where there appears to be imminent threat of serious harm. The Student Advocate’s Office (SAO) is an executive, non-partisan office of the ASUC. We offer free, confidential casework services and resources to any student(s) navigating issues with the University, including academic, conduct, financial aid, and grievance concerns. All support is centered around students and aims for an equity-based approach. For recovery from sexual harassment or sexual assault . The Care Line (510-643-2005) is a 24/7, confidential, free, campus-based resource for urgent support around sexual assault, sexual harassment, interpersonal violence, stalking, and invasion of sexual privacy. The Care Line will connect you with a confidential advocate for trauma-informed crisis support including time-sensitive information, securing urgent safety resources, and accompaniment to medical care or reporting. For social services . Social Services provides confidential services and counseling to help students with managing problems that can emerge from illness such as financial, academic, legal, family concerns, and more. They specialize in helping students with pregnancy resources and referrals; alcohol/drug problems related to one’s own or a family member’s use; sexual assault/rape; relationship or other violence; and support for health concerns-new diagnoses or ongoing conditions. Social Services staff will assess a student’s immediate needs, work with the student to develop a plan to meet those needs, and facilitate arrangements with academic departments and advocate for the student with other campus offices and community agencies, as well as coordinate services within UHS. For finding community on campus . The mission of the Berkeley International Office(BIO) (2299 Piedmont Avenue, 510-642-2818) is to provide support with all the essential resources needed to not only survive, but thrive here at UC Berkeley. Their mission is to support you and work together towards justice and belonging for all. They define Basic Needs as the essential resources that impact your health, belonging, persistence, and overall well being. It is an ecosystem that includes: nutritious food, stable housing, hygiene, transportation, healthcare, mental wellness, financial sustainability, sleep, and emergency dependent services. They refuse to accept hunger, homelessness, and all other basic needs injustices as part of our university. The Gender Equity Resource Center, fondly referred to as GenEq, is a UC Berkeley campus community center committed to fostering an inclusive Cal experience for all. GenEq is the campus location where students, faculty, staff and Alumni connect for resources, services, education and leadership programs related to gender and sexuality. The programs and services of the Gender Equity Resource Center are focused into four key areas: women; lesbian, gay, bisexual, and transgender (LGBT); sexual and dating violence; and hate crimes and bias driven incidents. GenEq strives to provide a space for respectful dialogue about sexuality and gender; illuminate the interrelationship of sexism, homophobia and gender bias and violence; create a campus free of violence and hate; provide leadership opportunities; advocate on behalf of survivors of sexual, hate, dating and gender violence; foster a community of women and LGBT leaders; and be a portal to campus and community resources on LGBT, Women, and the many intersections of identity (e.g., race, class, ability, etc.). The Undocumented Students Program (119 Cesar Chavez Center; 642-7224) practices a holistic, multicultural and solution-focused approach that delivers individualized service for each student. The academic counseling, legal support, financial aid resources and extensive campus referral network provided by USP helps students develop the unique gifts and talents they each bring to the university, while empowering a sense of belonging. The program’s mission is to support the advancement of undocumented students within higher education and promote pathways for engaged scholarship. The Multicultural Education Program (MEP) is one of six initiatives funded by the Evelyn and Walter Haas, Jr. Fund to work towards institutional change and to create a positive campus climate for diversity. The MEP is a five-year initiative to establish a sustainable infrastructure for activities like educational consultation and diversity workshops for the campus that address both specific topics, and to cater to group needs across the campus. For basic needs (food, shelter, etc.) . The Basic Needs Center (lower level of MLK Student Union, Suite 72) provides support with all the essential resources needed to not only survive, but thrive here at UC Berkeley. Their mission is to support you and work together towards justice and belonging for all. They define Basic Needs as the essential resources that impact your health, belonging, persistence, and overall well being. It is an ecosystem that includes: nutritious food, stable housing, hygiene, transportation, healthcare, mental wellness, financial sustainability, sleep, and emergency dependent services. They refuse to accept hunger, homelessness, and all other basic needs injustices as part of our university. The UC Berkeley Food Pantry (#68 Martin Luther King Student Union) aims to reduce food insecurity among students and staff at UC Berkeley, especially the lack of nutritious food. Students and staff can visit the pantry as many times as they need and take as much as they need while being mindful that it is a shared resource. The pantry operates on a self-assessed need basis; there are no eligibility requirements. The pantry is not for students and staff who need supplemental snacking food, but rather, core food support. ",
    "url": "/sp25/resources/#wellness-resources",
    
    "relUrl": "/resources/#wellness-resources"
  },"104": {
    "doc": "Staff",
    "title": "Staff",
    "content": " ",
    "url": "/sp25/staff/",
    
    "relUrl": "/staff/"
  },"105": {
    "doc": "Staff",
    "title": "Instructors",
    "content": "Professor Professor Aditya Parameswaran . He/Him . adityagp@berkeley.edu . ",
    "url": "/sp25/staff/#instructors",
    
    "relUrl": "/staff/#instructors"
  },"106": {
    "doc": "Staff",
    "title": "Lead Teaching Assistants",
    "content": "Student Support Assignments Christy Quang . SHE/HER . christyquang@berkeley.edu . Hi! I’m Christy, a senior studying Computer Science and Data Science. I know a lot about the Warriors. Excited to meet everyone! . Logistics Infrastructure Zackary Oon . HE/HIM . zackaryoon134@berkeley.edu . Hi, I’m Zack, and I’m a senior studying computer science and data science. I like playing super smash bros., and I’m trying to learn mandarin and drawing! . ",
    "url": "/sp25/staff/#lead-teaching-assistants",
    
    "relUrl": "/staff/#lead-teaching-assistants"
  },"107": {
    "doc": "Staff",
    "title": "Teaching Assistants",
    "content": "Grading Aditi Somayajula . SHE/HER . aditi.somayajula@berkeley.edu . Hi! I am a senior majoring in Computer Science. Looking forward to this semester! . Projects Li Zhang . SHE/HER . iris421@berkeley.edu . Hey I’m Li! I was a tutor&amp;reader for 101, and I’m so excited to see you guys in sections as a TA. I love going to the gym, watching anime, and singing. Feel free to reach out and chat! . Course Notes Michelle Lin . SHE/HER . lin.michelle@berkeley.edu . Hi, this is Michelle! I am a Double Bear majoring in MIMS at the School of Information, with an undergraduate degree in Data Science and Statistics. I’m also a huge cat lover! This is my second time teaching Data 101 and I can’t wait to meet y’all! . ",
    "url": "/sp25/staff/#teaching-assistants",
    
    "relUrl": "/staff/#teaching-assistants"
  },"108": {
    "doc": "Staff",
    "title": "Tutors",
    "content": "Aldrin Sembrana . HE/HIM . aldrinsembrana@berkeley.edu . Hi, I am a senior majoring in Data Science, and I enjoy dancing, eating, thrifting, cafe-ing, and more! I’m excited to meet you all :)) . Chenxi Qin . SHE/HER . chenxi1208@berkeley.edu . Hiii, I’m Stella (Chenxi)! I’m a junior studying Data Science &amp; CS. Super excited to be part of the course staff for Data 101 this semester, and looking forward to meeting y’all! . Manas Khatore . HE/HIM . manaskhatore@berkeley.edu . Hey everyone! I’m Manas and I’m a fourth-year studying data science and public policy. Hit me up if you wanna chat about AI policy, Pokémon, Bay Area restaurants, or Data 101! . Nehal Sindhu . SHE/HER . nehalsindhu@berkeley.edu . Hi! I’m a senior studying Data Science and Public Health and I’m super excited to teach data 101! I love pandas (the animal more than the library), matcha, concerts, and Thai food! . Wesley Zhang . HE/HIM . wesleyzhang357@berkeley.edu . Hi everyone, I’m Wesley. Looking forward to working with you all this semester! . ",
    "url": "/sp25/staff/#tutors",
    
    "relUrl": "/staff/#tutors"
  },"109": {
    "doc": "Syllabus",
    "title": "Syllabus",
    "content": " ",
    "url": "/sp25/syllabus/#syllabus",
    
    "relUrl": "/syllabus/#syllabus"
  },"110": {
    "doc": "Syllabus",
    "title": "Table of contents",
    "content": ". | About Data 101 (Info 258) . | Prerequisites | Info 258 vs Data 101 | Enrollment | Communications | . | Course Culture . | Be Aware of Your Actions | Be an Adult | Issues with Course Staff | Misuse of Course Resources | . | Course Components . | Lecture | Discussion | Projects | Homework Assignments | Exams | Final Project | Office Hours | . | Grading | Late Policy . | Additional Extensions | . | DSP Accommodations | Collaboration and Integrity | Academic and Wellness Resources | We want you to succeed! | . ",
    "url": "/sp25/syllabus/#table-of-contents",
    
    "relUrl": "/syllabus/#table-of-contents"
  },"111": {
    "doc": "Syllabus",
    "title": "About Data 101 (Info 258)",
    "content": "This course will cover the principles and practices of managing data at scale, with a focus on use cases in data analysis and machine learning. We will cover the entire life cycle of data management and science, ranging from data preparation to exploration, visualization and analysis, to machine learning and collaboration. The class will balance foundational concerns with exposure to practical languages, tools, and real-world concerns. We will study the foundations of prevalent data models in use today, including relations, tensors, and dataframes, and mappings between them. We will study SQL as a means to query and manipulate data at scale, including analytical challenges like sampling, aggregation and windowing, and performance concerns like views and indexes, data models, query processing and optimization, and transactions, all from a user perspective. We will study the foundations and realities of data preparation, including hands-on work with real-world data using standard Python and SQL frameworks. We will explore data exploration modalities for non-programmers, including the fundamentals behind spreadsheet systems and interactive visual analytics packages. We will look at approaches for managing the lifecycle of data science, including the establishment, monitoring, and maintenance of data pipelines for both analytics and machine learning. Time permitting, we will look at the specifics of ML pipelines including data validation, training, prediction serving and feedback loops, as well as technologies for representing, moving, sharing, and caching data including event streaming systems, key-value/document stores, in-memory and on-disk representation formats, log analytics, and search engines. Textbook: There is no official textbook for Data 101 this semester; we will provide (informal) course notes that will be released with the respective lectures. Prerequisites . COMPSCI C100/DATA C100/STAT C100 or COMPSCI 189 or INFO 251 or DATA 144/INFO 254 or equivalent upper-division course in data science. COMPSCI 61B or INFO 206B or equivalent courses in programming. This class will not assume deep experience with databases or big data solutions. Info 258 vs Data 101 . Data 101 is the same class as Info 258; there are some small differences with respect to the weightage for the optional final project (basically: this is optional for Data 101 but compulsory for Info 258) but everything else stays the same. Enrollment . See our FAQ page. Communications . | Ed is our primary method of communication and making announcements, and you are responsible for checking it frequently. We will also make assignment “megathreads” where you can public ask questions about the course assignments. | bCourses will only have lecture webcasts, if any. | Gradescope is where all assignments are submitted. | data101@berkeley.edu is the course staff email and is for private logistical student support and DSP accommodations. This email monitored by both the instructor and a core set of staff to ensure fastest response. Please only contact the course instructor directly for matters that require strict privacy and/or their personal attention. | . ",
    "url": "/sp25/syllabus/#about-data-101-info-258",
    
    "relUrl": "/syllabus/#about-data-101-info-258"
  },"112": {
    "doc": "Syllabus",
    "title": "Course Culture",
    "content": "Students taking Data C101 (Info 258) come from a wide range of backgrounds. We hope to foster an inclusive and safe learning environment based on curiosity rather than competition. All members of the course community — the instructors, students, and course staff — are expected to treat each other with courtesy and respect. Some of the responsibility for that lies with the staff, but a lot of it ultimately rests with you, the students. Be Aware of Your Actions . Sometimes, the little things add up to creating an unwelcoming culture to some students. For example, you and a friend may think you are sharing in a private joke about other races, majors, genders, abilities, cultures, etc. but this can have adverse effects on classmates who overhear it. There is a great deal of research on something called “stereotype threat”: research finds that simply reminding someone that they belong to a particular culture or share a particular identity (on whatever dimension) can interfere with their course performance. Stereotype threat works both ways: you can assume that a student will struggle based on who they appear to be, or you can assume that a student is doing great based on who they appear to be. Both are potentially harmful. Bear in mind that diversity has many facets, some of which are not visible. Your classmates may have medical conditions (physical or mental), personal situations (financial, family, etc.), or interests that aren’t common to most students in the course. Another aspect of professionalism is avoiding comments that (likely unintentionally) put down colleagues for situations they cannot control. Bragging in open space that an assignment is easy or “crazy,” for example, can send subtle cues that discourage classmates who are dealing with issues that you can’t see. Please take care, so we can create a class in which all students feel supported and respected. Be an Adult . Beyond the slips that many of us make unintentionally are a host of behaviors that the course staff, department, and university do not tolerate. These are generally classified under the term harassment; sexual harassment is a specific form that is governed by federal laws known as Title IX. UC Berkeley’s Title IX website provides many resources for understanding the terms, procedures, and policies around harassment. Make sure you are aware enough of these issues to avoid crossing a line in your interactions with other students. For example, repeatedly asking another student out on a date after they have said no can cross this line. Your reaction to this topic might be to laugh it off, or to make or think snide remarks about “political correctness” or jokes about consent or other things. You might think people just need to grow a thicker skin or learn to take a joke. This isn’t your decision to make. Research shows the consequences (emotional as well as physical) on people who experience harassment. When your behavior forces another student to focus on something other than their education, you have crossed a line. You have no right to take someone else’s education away from them. Issues with Course Staff . From the Data Science Department: Data Science Undergraduate Studies faculty and staff are committed to creating a community where every person feels respected, included, and supported. We recognize that incidents may happen, sometimes unintentionally, that run counter to this goal. There are many things we can do to try to improve the climate for students, but we need to understand where the challenges lie. If you experience a remark, or disrespectful treatment, or if you feel you are being ignored, excluded or marginalized in a course or program-related activity, please speak up. Consider talking to your instructor, but you are also welcome to contact Executive Director Christina Teller at cpteller@berkeley.edu or report an incident anonymously through this online form. As course staff, we are committed to creating a learning environment welcoming of all students that supports a diversity of thoughts, perspectives and experiences and respects your identities and backgrounds (including race, ethnicity, nationality, gender identity, socioeconomic class, sexual orientation, language, religion, ability, and more.) To help accomplish this: . | If your name and/or pronouns differ from those that appear in your official records, please let us know. | If you feel like your performance in the class is being affected by your experiences outside of class (e.g., family matters, current events), please don’t hesitate to come and talk with us. We want to be resources for you. | We (like many people) are still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to us about it. | While the course staff understands that improving diversity, equity, and inclusion (DEI) are not enough to overcome systemic issues in academia such as racism, queerphobia, and other forms of discrimination and hatred, we also recognize the importance of DEI work. | The Data Science Department has some resources available at https://data.berkeley.edu/about/diversity-equity-and-inclusion. | There’s also a great set of resources available at https://eecs.berkeley.edu/resources/students/grievances. | . | If there are other resources you think we should list here, let us know! | . We will take all complaints about unprofessional or discriminatory behavior seriously. Misuse of Course Resources . If individuals are disrespectful to students, course staff, or others via course resources, they will lose access to course resources. E.g., if someone is unkind in the course forum, their account will be removed from the course forum. If someone is unkind in the classroom, they will be asked to leave the classroom. ",
    "url": "/sp25/syllabus/#course-culture",
    
    "relUrl": "/syllabus/#course-culture"
  },"113": {
    "doc": "Syllabus",
    "title": "Course Components",
    "content": "Lecture . There are two 80-minute lectures per week, TuTh 9:30am, Lewis 100. You can attend in-person or online, or watch the recording. The class schedule will contain links to video recordings posted within 24 hours after the corresponding class. Lecture check-ins: Every lecture (starting the 3rd lecture, 1/28, onwards) will have a short series of questions to help check your understanding of the material. You will have one week to complete each check-in; no late submissions will be accepted. Each check-in is worth 1 point and is graded on completeness as follows: . | Synchronous (during lecture) complete at least one question. | Asynchronous (after lecture, within 1 week) complete all questions. | . In both cases, navigate to slido.com . Discussion . This course also includes one weekly 50-minute discussion section held on Thursdays and Fridays. This section will help you synthesize materials via worksheets and activities and is good practice for the final exam. Handouts wil be posted on the class Schedule. Section attendance is not required, but you are strongly encouraged to practice the material on your own time. Discussion begins with the first week of classes. Projects . There will be five (5) programming assignments released throughout the semester (see the schedule). Generally, you will have one to two weeks to complete each project. Projects are graded on accuracy and are almost equally weighted, with the exception of Project 0 (weighted half of the other projects). Project deadlines are generally Wednesday 5pm. Homework Assignments . There will be five (5) homework assignments released on Gradescope throughout the semester. You will have one to two weeks to complete short written assignments. Each homework assignment consists of multiple choice and short answer questions. Homework assignments are graded on accuracy and are equally weighted. Homework assignment deadlines are generally Wednesday 5pm. Exams . This course has two exams: . | Midterm exam: Wednesday, March 12th, 6:00pm - 8:00pm | Final exam: Wednesday, May 14th, 11:30am - 2:30pm | . Exams are offered in-person only. It is your responsibility to ensure that you are not enrolled in another class that conflicts with our midterm or final exam time. All details will be posted on Ed. Final Project . We will have an optional final course project this semester. This will be an open-ended exploration of data systems, and you will submit a written final report. Details to be released after the midterm. Office Hours . Office Hours are a great place to get help from course staff and to find study partners for this course. You can find a list of all office hours on the calendar page. There are two categories of office hours: . | TA/Tutor office hours, held weekly in Warren 101B. Attend to get help with homework assignments, projects, or any other content-related questions. | Instructor office hours, held weekly in Soda 787 are for content-related questions, but also general course questions, data science advising, or perspectives on academia/industry. These are not intended for homework or project questions. Feel free to bring tea/coffee and/or a reusable mug. | . ",
    "url": "/sp25/syllabus/#course-components",
    
    "relUrl": "/syllabus/#course-components"
  },"114": {
    "doc": "Syllabus",
    "title": "Grading",
    "content": "Letter grades for the course will be based on your overall score in the class. The default scheme for Data 101 is without the optional final project: . | Component | Weight(default) | Weight(Final Project) | Details | . | Lecture Check-Ins | 6% | 6% | Drop 4 lowest scores. Skip Lecture 01 and 02. | . | Projects | 36% | 36% | (8% each typically) No drops; Project 0 worth half others. see Late Policy | . | Homework Assignments | 18% | 18% | (3.6% each) No drops; see Late Policy | . | Midterm | 15% | 10% |   | . | Final Exam | 25% | 20% | Final is cumulative. | . | Final Project | – | 10% | Optional | . If you choose to do the project, your final grade will be the maximum of the two grading schemes. For Info 258 students, the grading scheme will be the one involving the final project. ",
    "url": "/sp25/syllabus/#grading",
    
    "relUrl": "/syllabus/#grading"
  },"115": {
    "doc": "Syllabus",
    "title": "Late Policy",
    "content": "Everyone has 9 slip days, which can be applied to late submissions of projects and homework assignments, including the final project. | Each project or homework assignment can have a max of 3 slip days applied to it. These slip days will be automatically applied at the end of the semester to maximize your grade, based on the final grade shown in gradescope. | Slip days are rounded up to the next day. That is, 5 minutes late = 1 day late. We will use the submission time as displayed on Gradescope. You should track your own slip days. | If you have no remaining slip days, there is a 15% reduction to your total score for every day that the assignment is late. After 3 slip days (i.e., late days), you can no longer receive credit for the submission. | Lecture check-ins are not eligible for slip days. | Please do not ask the staff to apply slip days to particular assignments, or track how many you have used. If an assignment is open for submission it is always in your interest to turn in your best effort. | . Additional Extensions . We recognize that life can be unexpected. If you encounter the need for additional extensions at any time in the semester, please don’t hesitate to let us know. The sooner we are made aware, the more options we have available to us to help you. The Additional Extensions form is for addressing any circumstances that cannot be resolved via the slip day policy above. This form is designed to lower the barrier to reaching out to us, as well as build your independence in managing your academic career long-term. Within one business day of filling out the form, a course staff will reach out to you and, as needed, provide a space for conversation and arrange course accommodations as necessary. When making extension requests: . | Send requests before the assignment deadline | Send one extension request per assignment | If your extension request is granted, then the extension is to the original on-time deadline, from which slip days can be used. We will prioritize using your slip days over providing additional extensions. | . You are responsible for reasonable, timely communication with course staff. Simply submitting a request does not guarantee you will receive an extension. Even if your work is incomplete, please submit before the deadline so you can receive credit for the work you did complete. ",
    "url": "/sp25/syllabus/#late-policy",
    
    "relUrl": "/syllabus/#late-policy"
  },"116": {
    "doc": "Syllabus",
    "title": "DSP Accommodations",
    "content": "If you have DSP accommodations for extended time on assignments, please make sure we have received your DSP accommodation letter. We will contact you about arrangements. ",
    "url": "/sp25/syllabus/#dsp-accommodations",
    
    "relUrl": "/syllabus/#dsp-accommodations"
  },"117": {
    "doc": "Syllabus",
    "title": "Collaboration and Integrity",
    "content": "You are encouraged to discuss practice problems and lecture content with your fellow students and with course staff. Arguing with friends about exercises is an excellent and time-honored way to learn. However, you must write up all your own assignments and code by yourself. Copying assignments from other sources is not only dishonest, it also doesn’t help anyone, least of all yourself. We have important rules: . | All code you submit should be written by you alone. | Do not possess or share code. Before submitting your final work, you should never be in possession of solution code that you did not write. Looking up solution code online is effectively possessing solution code. You will be equally culpable if you distribute code, now or in the future. If you find yourself struggling or really desperate, reach out to the staff and/or submit to Additional Extensions. | Cite your sources: . | Study groups: If you do discuss the assignments with others please include their names at the top of your notebook. Restated, you and your friends are encouraged to discuss course content and high-level approaches to problem-solving, but you are not allowed to share your code nor answers. You can work on a project alongside another person or group of people (e.g., study group), but it should not substantially resemble anyone else’s. | StackOverflow, etc. You should cite these sources, even if it’s using small snippets of code (e.g., googling “postgres string matching” may lead you to some sample code that you copy and paste. Include the link to these online sources. | With extreme caution: If you’re just generating some amount of boilerplate code with GitHub Copilot / LLMs / etc., that’s OK. However, you should not use such tools to generate non-trivial methods. AI methods often suffer from hallucinations, incompleteness, mistakes, and other idiosyncracies, and so there is a real danger to trusting them. There is always a need for human verification and validation, so in this class we emphasize your ability to do data engineering work without access to AI. Any AI-generated code must be cited (with the prompt and log) and explicitly indicated as such. Violation of this citation rule is a serious act of plagiarism. | . | . Each assignment will ask you to list any collobrators, outside sources, or other help received. You are expected to fill this hout honestly. We will follow the EECS departmental policy on academic honesty, so be sure you are familiar with it. We will be running advanced plagiarism detection programs on all assignments. Posting solutions is also prohibited. If you find a solution online, please email a link to that solution tot he instructor. We are tough with dishonest students and we hope that we will not be put in that situation in this class. We expect that you will work with integrity and with respect for other members of the class, just as the course staff will work with integrity and with respect for you. If you need help, just reach out and ask us. You are not alone. ",
    "url": "/sp25/syllabus/#collaboration-and-integrity",
    
    "relUrl": "/syllabus/#collaboration-and-integrity"
  },"118": {
    "doc": "Syllabus",
    "title": "Academic and Wellness Resources",
    "content": "Our Resources page lists not only course-specific academic resources such as course notes, past exams, study guides, and prerequisite review links, but also campus wellness resources on COVID-19, academic support, technology support, mental well-being, DSP accommodations, dispute resolution, social services, campus community, and basic needs. Our staff will also refer to this page when supporting you through this course. ",
    "url": "/sp25/syllabus/#academic-and-wellness-resources",
    
    "relUrl": "/syllabus/#academic-and-wellness-resources"
  },"119": {
    "doc": "Syllabus",
    "title": "We want you to succeed!",
    "content": "If you are feeling overwhelmed, visit our office hours and talk with us, or fill out the Additional Extensions Form. We know college can be stressful and we want to help you succeed. Finally, the main goal of this course is that you should learn, and have a fantastic experience doing so. Please keep that goal in mind throughout the semester. Welcome to Data 101! . ",
    "url": "/sp25/syllabus/#we-want-you-to-succeed",
    
    "relUrl": "/syllabus/#we-want-you-to-succeed"
  },"120": {
    "doc": "Syllabus",
    "title": "Syllabus",
    "content": " ",
    "url": "/sp25/syllabus/",
    
    "relUrl": "/syllabus/"
  }
}
